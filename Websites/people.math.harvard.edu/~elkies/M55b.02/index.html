<TITLE>
Math 55b: Honors Advanced Calculus and Linear Algebra (Spring 200[2-]3)
</TITLE>

<BODY BACKGROUND="calabi1.gif">

<STRONG>
Lecture notes, etc., for
Math 55b: Honors Advanced Calculus and Linear Algebra
(Spring 200[2-]3)
</STRONG>

<P>

If you find a mistake, omission, etc., please
<A HREF="mailto:elkies@math.harvard.edu">let me know</A>
by e-mail.

<blockquote>
  <img src="purpball.gif">
  <A HREF="http://www.fas.harvard.edu/~jorza/math55">Andrei's
  Math 55 page</A>
  <br>
  <img src="purpball.gif">
  <A HREF="qanda.html">Q & A</A>:
   Questions that arose concerning lectures, problem sets, etc.,
   and my replies
</blockquote>

<P>

The <FONT COLOR="#B87800">orange</FONT COLOR> balls
mark our <A HREF="#today">current location</A> in the course,
and the <A HREF="#current_homework">current problem set</A>.

<HR>

<img src="redball.gif">
We start with <em>differential calculus</em> of vector-valued functions
of one real variable, building on Chapter 5 of Rudin.
<blockquote>
You may have already seen ``little oh'' and ``big Oh'' notations.
For functions <em>f</em>, <em>g</em> on the same space,
``<em>f</em>=<em>O</em>(<em>g</em>)'' means that
<em>g</em> is a nonnegative real-valued function,
<em>f</em> takes values in a normed vector space,
and there exists a real constant <em>M</em> such that
|<em>f</em>(<em>x</em>)|&lt;=<em>Mg</em>(<em>x</em>)
for all <em>x</em>.  The notation ``<em>f</em>=<em>o</em>(<em>g</em>)''
is used in connection with a limit; for instance,
``<em>f</em>(<em>x</em>)=<em>o</em>(<em>g</em>(<em>x</em>))
as <em>x</em> approaches <em>x</em><sub>0</sub>'' indicates that
<em>f</em>, <em>g</em> are vector- and real-valued functions as above
on some neighborhood of <em>x</em><sub>0</sub>, and that for each
<em>c</em>&gt;0 there is a neighborhood of <em>x</em><sub>0</sub>
such that |<em>f</em>(<em>x</em>)|&lt;=<em>cg</em>(<em>x</em>)
for all <em>x</em> in the neighborhood.  Thus
<em>f</em>'(<em>x</em><sub>0</sub>)=<em>a</em> means the same as
``<em>f</em>(<em>x</em>)=<em>f</em>(<em>x</em><sub>0</sub>)+<em>a</em>(<em>x</em>-<em>x</em><sub>0</sub>)+<em>o</em>(|<em>x</em>-<em>x</em><sub>0</sub>|)
as <em>x</em> approaches <em>x</em><sub>0</sub>'',
with no need to exclude the case <em>x</em>=<em>x</em><sub>0</sub>.
Rudin in effect uses this approach when proving the Chain Rule (5.5).
<p>
In Rudin's proof of L'H&ocirc;pital's Rule (5.13),
why can he assume that <em>g</em>(<em>x</em>)
does not vanish for any <em>x</em> in (<em>a</em>,<em>b</em>),
and that the denominator <em>g</em>(<em>x</em>)-<em>g</em>(<em>y</em>)
in equation (18) is never zero?
<p>
The derivative of <em>f</em>/<em>g</em> can be obtained from
the product rule, together with the derivative of 1/<em>g</em>
-- which in turn can be obtained from the Chain Rule together
with the the derivative of the single function 1/<em>x</em>.
Once we do multivariate differential calculus,
we'll see that the derivatives of
  <em>f</em>+<em>g</em>, <em>f</em>-<em>g</em>,
  <em>f</em><em>g</em>, <em>f</em>/<em>g</em>
could also be obtained in much the same way
that we showed the continuity of those functions,
by combining the multivariate Chain Rule
with the derivatives of the specific functions
  <em>x</em>+<em>y</em>, <em>x</em>-<em>y</em>,
  <em>x</em><em>y</em>, <em>x</em>/<em>y</em>
of two variables <em>x</em>,<em>y</em>.
<p>
As Rudin notes at the end of this chapter, differentiation can also
be defined for vector-valued functions of one real variable.  As Rudin
does not note, the vector space can even be infinite-dimensional,
provided that it is normed; and the basic algebraic properties of the
derivative listed in Thm. 5.3 (p.104) can be adapted to this generality,
e.g., the formula <EM>(fg)'=f'g+fg'</EM> still holds if <EM>f,g</EM>
take values in normed vector spaces <EM>U,V</EM> and multiplication
is interpreted as a continuous bilinear map from <EM>U x V</EM>
to some other normed vector space <EM>W</EM>.

<P>

NB The norm does not have to come from an inner product structure.
Often this does not matter because we work in finite dimensional
vector spaces, where all norms are equivalent, and changing to
an equivalent norm does not affect the definition of the derivative.
The one exception to this is Thm. 5.19 (p.113) where one needs the
norm exactly rather than up to a constant factor.  This theorem still
holds for a general norm but requires an additional argument.
<A NAME="HahnBanach">
The key ingredient of the proof is this: given a nonzero vector
<EM>z</EM> in a vector space <EM>V</EM>, we want a continuous
functional <EM>w</EM> on <EM>V</EM> such that ||<EM>w</EM>||=1
and <EM>w(z)</EM>=|<EM>z</EM>|.  If <EM>V</EM> is an inner product
space (finite-dimensional or not), the inner product with
<EM>z</EM>/|<EM>z</EM>| provides such a functional <EM>w</EM>.
But this approach does not work in general.  The existence of
such <EM>w</EM> is usually proved as a corollary of the Hahn-Banach
theorem.  When <EM>V</EM> is finite dimensional, <EM>w</EM> can
be constructed by induction on the dimension of <EM>V</EM>.
To deal with the general case one must also invoke the Axiom of Choice
in its usual guise of Zorn's Lemma.

</blockquote>

<img src="redball.gif">
We next start on <STRONG>univariate integral calculus</STRONG>,
following Rudin, chapter 6.
The following gives some motivation for the definitions there.
(And yes, it's the same Riemann who gave number theorists like me
the Riemann zeta function and the Riemann Hypothesis.)

<blockquote>

The Riemann-sum approach to integration goes back to the
``method of exhaustion'' of classical Greek geometry,
in which the area of a plane figure (or the volume of a region in space)
is bounded below and above by finding subsets and supersets
that are finite unions of disjoint rectangles (or boxes).
The lower and upper Riemann sums adapt this idea
to the integrals of functions which may be negative as well as positive
(one of the weaknesses of geometric Greek mathematics is that
the ancient Greeks had no concept of negative quantities
-- nor, for that matter, of zero).
<A NAME="quadrature">
You may have encountered the quaint technical term ``quadrature'',
used in some contexts as a synonym for ``integration''.
This too is an echo of the geometrical origins of integration.
``Quadrature'' literally means ``squaring'', meaning not
``multiplying by itself'' but ``constructing a square of
the same size as''; this in turn is equivalent to
``finding the area of'', as in the phrase ``squaring the circle''.
For instance, Greek geometry contains a theorem equivalent
to the integration of <EM>x</EM><SUP>2</SUP><EM>dx</EM>,
a result called the ``quadrature of the parabola''.
The proof is tantamount to the evaluation of lower and
upper Riemann sums for the integral of
<EM>x</EM><SUP>2</SUP><EM>dx</EM>.

<P>

An alternative explanation of the upper and lower Riemann sums
is that they arise by repeated application of the following two
axioms describing the integral:
<UL>
<LI> For any <EM>a,b,c</EM> (with <EM>a &lt; b &lt; c</EM>),
the integral of a function from <EM>a</EM> to <EM>c</EM>
is the sum of its integrals
from <EM>a</EM> to <EM>b</EM> and from <EM>b</EM> to <EM>c</EM>;
<LI> If a function <EM>f</EM> on [<EM>a,b</EM>] takes values in
[<EM>m,M</EM>] then its integral from <EM>a</EM> to <EM>b</EM>
is in [<EM>m</EM>(<EM>b-a</EM>), <EM>M</EM>(<EM>b-a</EM>)]
(again assuming <EM>a &lt;b</EM>).
</UL>
The latter axiom is a consequence of the following two: the integral of
a constant function from <EM>a</EM> to <EM>b</EM> is that constant
times the length <EM>b-a</EM> of the interval [<EM>a,b</EM>];
and if <EM>f</EM>&lt;=<EM>g</EM> on some interval
then the integral of <EM>f</EM> over that interval
does not exceed the integral of <EM>g</EM>.
Note that again all these axioms arise naturally from an interpretation
of the integral as a ``signed area''.

<P>

Recall the ``integration by parts'' identity: <EM>fg</EM>
is an integral of <EM>f dg + g df</EM>.  The Stieltjes integral
is a way of making sense of this identity even when <EM>f</EM>
and/or <EM>g</EM> is not continuously differentiable.  To be
sure, some hypotheses on <EM>f</EM> and <EM>g</EM> must still
be made for the Stieltjes integral of <EM>f dg</EM> to make sense.
Rudin specifies one suitable system of such hypotheses.

<P>

Here's a version of Riemann-Stieltjes integrals that works cleanly
for integrating bounded functions from [<EM>a</EM>,<EM>b</EM>]
to any complete normed vector space:
<A HREF="vint.ps">PS</A>, <A HREF="vint.pdf">PDF'</A>.

<P>

<EM>Riemann-Stieltjes integration by parts</EM>: Suppose both
<EM>f</EM> and <EM>g</EM> are increasing functions on
[<EM>a,b</EM>].  For any partition
<EM>a = x</EM><SUB>0</SUB></EM> &lt; ... &lt; <EM>x<SUB>n</SUB> = b </EM>
of the interval, write 
<EM>f</EM>(<EM>b</EM>)<EM>g</EM>(<EM>b</EM>)
- <EM>f</EM>(<EM>a</EM>)<EM>g</EM>(<EM>a</EM>)
as the telescoping sum of
<EM>f</EM>(<EM>x<SUB>i</SUB></EM>)<EM>g</EM>(<EM>x<SUB>i</SUB></EM>) -
<EM>f</EM>(<EM>x</EM><SUB><EM>i</EM>-1</SUB>)<EM>g</EM>(<EM>x</EM><SUB><EM>i</EM>-1</SUB>)
from <EM>i</EM>=1 to <EM>n</EM>.  Now rewrite the <EM>i</EM>-th summand
as
<center>
<EM>f</EM>(<EM>x<SUB>i</SUB></EM>) (<EM>g</EM>(<EM>x<SUB>i</SUB></EM>)-<EM>g</EM>(<EM>x</EM><SUB><EM>i</EM>-1</SUB>)) +
<EM>g</EM>(<EM>x</EM><SUB><EM>i</EM>-1</SUB>) (<EM>f</EM>(<EM>x<SUB>i</SUB></EM>)-<EM>f</EM>(<EM>x</EM><SUB><EM>i</EM>-1</SUB>)).
</center>
[Naturally it is no accident that this identity resembles the
one used in the familiar proof of the formula for the derivative of
<EM>fg</EM>&nbsp;!]  Summing this over <EM>i</EM> yields the upper
Riemann-Stieltjes sum for the integral of <EM>f dg</EM> plus
the lower R.-S. sum for the integral of <EM>g df</EM>.  Therefore:
if one of these integrals exists, so does the other, and their sum is
<EM>f</EM>(<EM>b</EM>)<EM>g</EM>(<EM>b</EM>)
- <EM>f</EM>(<EM>a</EM>)<EM>g</EM>(<EM>a</EM>).
[Cf. Rudin, page 141, Exercise 17.]

</blockquote>

<img src="redball.gif">
Most of <STRONG>Chapter 7</STRONG> of Rudin we've covered already
in the 55a lectures and problem sets.  For more counterexamples
along the lines of the first section of that chapter, see
<EM>Counterexamples in Analysis</EM> by B.R.Gelbaum and J.M.H.Olsted --
there are two copies in the science center library (QA300.G4).
Concerning Thm. 7.16, be warned that it can easily fail for
``improper integrals'' on infinite intervals
(see Rudin, p.138, Exercise 8, assigned on PS2).
It is often very useful
to bring a limit or an infinite sum within an integral sign,
but this procedure requires justification beyond Thm.&nbsp;7.16.

<p>

After covering a few odds and ends from Chapter 7, we'll proceed to
<STRONG>power series</STRONG> and the exponential and logarithmic
functions in <STRONG>Chapter 8</STRONG>; we'll use these to derive the
binomial theorem for arbitrary exponents.  This will give us a way to
simplify a key ingredient in the
<STRONG>Stone-Weierstrass theorem</STRONG>,
which is the one major result of Chapter 7 we haven't seen yet.
We postpone discussion of Euler's Beta and Gamma integrals
(also in Chapter 8) so that we can use multivariate integration to
give a more direct proof of the formula relating them.

<P>

The result concerning the convergence of alternating series
is stated and proved on pages
70-71 of Rudin (Theorem 3.42).

<P>

The original Weierstrass approximation theorem (7.26 in Rudin)
can be reduced to the uniform approximation of the single function
|<EM>x</EM>| on [-1,1].  From this function we can construct an
arbirtrary piecewise linear continuous function, and such P.L.
functions uniformly approximate any continuous function on a
closed interval.  To get at |<EM>x</EM>|, we'll rewrite it as
[1-(1-<EM>x</EM><SUP>2</SUP>)]<SUP>1/2</SUP>, and use the
power series for (1-<EM>X</EM>)<SUP>1/2</SUP>.  See the first part of
exercise 22 for chapter 8, on p.201, for this power series (and
more generally for the power series for
(1-<EM>X</EM>)<SUP><EM>a</EM></SUP>).
We need (1-<EM>X</EM>)<SUP>1/2</SUP> to be approximated by its
power series uniformly on the <EM>closed</EM> interval [-1,1]
(or at least [0,1]); but fortunately this too follows from the proof
of Abel's theorem (8.2, pages 174-5).  Actually this is a subtler
result than we need, since the <EM>X<SUP>n</SUP></EM> coefficient
of the power series for (1-<EM>X</EM>)<SUP>1/2</SUP> is negative
for every <EM>n</EM>&gt;0.  If a power series in <EM>X</EM> has
radius of convergence 1 and all but finitely many of its nonzero
coefficients have the same sign, then it is easily shown that the sum
of the coefficients converges if and only if <EM>f</EM>(<EM>X</EM>) has
a finite limit as <EM>X</EM> approaches 1, in which case the sum
equals that limit and the power series converges uniformly on [0,1].
That's all we need because clearly (1-<EM>X</EM>)<SUP>1/2</SUP>
extends to a continuous function on [0,1].  (For an alternative approach
to uniformly approximating |<EM>x</EM>|, see exercise 23 on p.169.)

<P>

An explicit upper bound of 4 on Pi can be obtained by calculating
cos(2) &lt; 1 - 2^2/2! + 2^4/4! = -1/3 &lt; 0.
For much better estimates, integrate
(<EM>x-x</EM><SUP>2</SUP>)<SUP>4</SUP>
<EM>dx</EM>/(<EM>x</EM><SUP>2</SUP>+1) from 0 to 1 and note that
0 &lt; 1/(<EM>x</EM><SUP>2</SUP>+1) &lt;= 1.  :-)

<!--
<P>

The proof we outlined for the analyticity of
(1-<em>x</em>)<sup><em>a</em></sup> on |<em>x</em>|&lt;1
is Exercise 22 on p.201 of Rudin.
-->

<P>

Rudin uses a fact about convex functions that is only presented
as an exercise earlier in the book (p.100, #23).  Namely:
let <EM>f</EM> be a convex function on some interval <EM>I</EM>,
and consider <EM>s</EM>(<EM>x</EM>, <EM>y</EM>) :=
(<EM>f</EM>(<EM>x</EM>)-<EM>f</EM>(<EM>y</EM>)) / (<EM>x-y</EM>)
as a function on the set of (<EM>x</EM>, <EM>y</EM>) in
<EM>I</EM> x <EM>I</EM> with <EM>x</EM> > <EM>y</EM>;
then <EM>s</EM> is an increasing function of both variables.
The proof is fortunately not hard.  For instance, to prove that if
<EM>x</EM> > <EM>y'</EM> > <EM>y</EM> then
<EM>s</EM>(<EM>x</EM>, <EM>y'</EM>) > <EM>s</EM>(<EM>x</EM>,
<EM>y</EM>),
write <EM>y'</EM> as <EM>px</EM>+<EM>qy</EM> and calculate that
<EM>s</EM>(<EM>x</EM>, <EM>y'</EM>) > <EM>s</EM>(<EM>x</EM>,
<EM>y</EM>)
is equivalent to the usual convexity condition.  The case
<EM>x</EM> > <EM>x'</EM> > <EM>y</EM> works in exactly the same way.

<P>

The rather obscure integration by parts in Rudin, p.194 is not necessary.
A straightfoward choice of ``parts'' yields
<P>
<center>
<EM>x</EM> B(<EM>x</EM>, <EM>y</EM>+1) =
<EM>y</EM> B(<EM>x</EM>+1, <EM>y</EM>) ;
</center>
<P>
This may seem to go in a useless direction,
but the elementary observation that
<P>
<center>
B(<EM>x</EM>, <EM>y</EM>) =
B(<EM>x</EM>, <EM>y</EM>+1) + B(<EM>x</EM>+1, <EM>y</EM>)
</center>
<P>
recovers the recursion (97).

<p>

In addition to the trigonometric definite integrals noted by Rudin
(formula 98), Beta functions also turn up in the evaluation of
the definite integral of
<EM>u<SUP> a</SUP> du</EM> /
(1+<EM>u<SUP> b</SUP></EM>)<SUP><EM>c</EM></SUP> over (0,infinity):
let <EM>t</EM> = <EM>u<SUP> b</SUP></EM> / (1+<EM>u<SUP> b</SUP></EM>).
What is the value of that integral?  Can you obtain in particular
the formula Pi / (<EM>b</EM> sin(<EM>a</EM> Pi/<EM>b</EM>)) for the
special case <EM>c</EM>=1?

<P>

The limit formula for Gamma(<em>x</em>) readily yields
the product formula:
<blockquote>
Gamma(<em>x</em>) = 
<em>x</em><sup>-1</sup> <em>e</em><sup>-<em>Cx</em></sup>
Prod(exp(<em>x</em>/<em>k</em>) / (1+<em>x</em>/<em>k</em>),
<em>k</em>=1,2,3,...)
</blockquote>
where <em>C</em>=0.57721566490... is <em>Euler's constant</em>,
the limit as <em>N</em> goes to infinity of
1+(1/2)+(1/3)+...+(1/<em>N</em>)-log(<em>N</em>).
This lets us easily show that Gamma is infinitely differentiable
(in fact analytic) and to obtain nice formulas for the derivatives
of log(Gamma(<em>x</em>)); for instance, Gamma'(1)=-<em>C</em>,
and more generally the logarithmic derivative of Gamma(<em>x</em>) at
<em>x</em>=<em>N</em>+1 is 1+(1/2)+(1/3)+...+(1/<em>N</em>)-<em>C</em>.

<P>

<img src="redball.gif">

We next begin <STRONG>multivariate differential calculus</STRONG>,
starting at the middle of Rudin Chapter 9 (since the first part
of that chapter is for us a review of linear algebra).  Again,
Rudin works with functions from open subsets of
<STRONG>R</STRONG><SUP><EM>n</EM></SUP>
to <STRONG>R</STRONG><SUP><EM>m</EM></SUP>,
but must of the discussion works equally well with the target space
<STRONG>R</STRONG><SUP><EM>m</EM></SUP> replaced by an arbitrary
normed vector space <EM>V</EM>.  If we want to allow arbitrary
normed vector spaces for the domain of <em>f</em>, we'll usually
have to require that the derivative <em>f</em>' be
a <em>continuous</em> linear map, or equivalently that its norm
||<em>f</em>'||=sup<sub>|<em>v</em>|&lt=1</sub>|<em>f</em>'(<em>v</em>)|
be finite.

<p>

As in the
<A HREF="#HahnBanach">univariate case</A>,
proving the Mean Value Theorem in the multivariate context
(Theorem 9.19) requires either that <EM>V</EM> have an inner-product
norm, or the use of the Hahn-Banach theorem
to construct suitable functionals on <EM>V</EM>.  Once this is done,
the key Theorem 9.21 can also be proved for functions to <EM>V</EM>,
and without first doing the case <EM>m</EM>=1.  To do this,
first prove the result in the special case when each
<EM>D<SUB>j</SUB> f</EM>(<STRONG>x</STRONG>)
vanishes; then reduce to this case by subtracting from
<EM>f</EM> the linear map from <STRONG>R</STRONG><SUP><EM>n</EM></SUP>
to <EM>V</EM> indicated by the partial derivatives
<EM>D<SUB>j</SUB> f</EM>(<STRONG>x</STRONG>).

<p>

The <EM>Inverse function theorem</EM> (9.24) is a special case
of the <EM>Implicit function theorem</EM> (9.28), and its
proof amounts to specializing the proof of the implicit function
theorem.  But Rudin proves the Implicit theorem as a special
case of the Inverse theorem, so we have to do Inverse first.
(NB for these two theorems we will assume
that our target space is finite-dimensional;
how far can you generalize to infinite-dimensional spaces?)
Note that Rudin's statement of the contraction principle
(Theorem 9.23 on p.220) is missing the crucial hypothesis
that <em>X</em> be nonempty!

<p>

The proof of the second part of the implicit function theorem,
which asserts that the implicit function <STRONG>g</STRONG> not only
exists but is also continuously differentiable with derivative
at <STRONG>b</STRONG> given by formula (58) (p.225), can be done
more easily using the chain rule, since <STRONG>g</STRONG> has been
constructed as the composition of the following three functions:
first, send <STRONG>y</STRONG> to (<STRONG>0,y</STRONG>); then,
apply the inverse function <STRONG>F</STRONG><SUP>-1</SUP>;
finally, project the resulting vector (<STRONG>x,y</STRONG>)
to <STRONG>x</STRONG>.  The first and last of these three functions
are linear, so certainly <EM>C</EM><SUP>1</SUP>; and the continuous
differentiability of <STRONG>F</STRONG><SUP>-1</SUP> comes from
the inverse function theorem.

<p>

Here's an approach to <EM>D<SUB>ij</SUB>=D<SUB>ji</SUB></EM>
that works for a <EM>C</EM><SUP>2</SUP> function to an arbitrary
normed space.  As in Rudin (see p.235) we reduce to the case of
a function of two variables, and define <EM>u</EM> and Delta.
Assume first that <EM>D</EM><SUB>21</SUB> <EM>f</EM> vanishes
at (<EM>a,b</EM>).  Then use the Fundamental Theorem of Calculus
to write Delta(<EM>f,Q</EM>) as as the integral of
<EM>u'</EM>(<EM>t</EM>)<EM>dt</EM> on [<EM>a</EM>,<EM>a+h</EM>],
and then write <EM>u'</EM>(<EM>t</EM>) as an integral of 
<EM>D</EM><SUB>21</SUB> <EM>f</EM>(<EM>t,s</EM>)<EM>ds</EM>
on [<em>b</em>,<em>b</em>+<EM>k</EM>].  Conclude that
<EM>u'</EM>(<EM>t</EM>)=<EM>o</EM>(<EM>k</EM>)
and thus that Delta(<EM>f,Q</EM>) / <EM>hk</EM> approaches zero.  
Now apply this to the function
<EM>f</EM>-<EM>xyD</EM><SUB>21</SUB> <EM>f</EM>(<EM>x,y</EM>)
to see that in general Delta(<EM>f,Q</EM>) / <EM>hk</EM> approaches
<EM>D</EM><SUB>21 </SUB><EM>f</EM>(<EM>x,y</EM>).  Do the same
in reverse order to conclude that
<EM>D</EM><SUB>21</SUB>
<EM>f</EM>(<EM>x,y</EM>)=<EM>D</EM><SUB>12</SUB> <EM>f</EM>(<EM>x,y</EM>).
Can you prove
<em>D</em><SUB>12</SUB>(<em>f</em>)=<em>D</em><SUB>21</SUB>(<em>f</em>)
for a function <em>f</em> to an arbitrary inner product space
under the hypotheses of Theorem 9.41?

<p>

<img src="redball.gif">
Next topic, and last one from Rudin, is
<STRONG>multivariate integral calculus</STRONG> (Chapter 10).
Having defined multivariate integrals and obtained
the formula for change of variables, we're setting up the machinery
of differential forms that is the framework for the higher-dimensional
generalization of the Fundamental Theorem of Calculus that comprises
the divergence, Stokes, and Green theorems and much else besides.

<p>

The basic formula (92) that yields generalized Stokes (formula 91)
is even more easily proved for oriented <EM>k</EM>-cells instead of
<EM>k</EM>-simplices, since each term of <EM>d</EM>(omega) on the
left-hand side of the identity yields the sum of 2 of the 2<EM>k</EM>
terms of the boundary occurring on the right-hand side.  Naturally the
signs that enter when one does it for a <EM>k</EM>-simplex are the
reason we put the (-1)<SUP><EM>j</EM></SUP> into the definition of
the boundary (formula 85).  One interpretation of generalized Stokes
is that the operators <EM>d</EM> on differential forms, and boundary
on chains, are adjoints relative to the pairing given by integration.
In fact just as <EM>d</EM><SUP> 2</SUP>=0 we have
(boundary)<SUP>2</SUP>=0; this is easy to show, and further
bolsters our confidence in our choice of signs in formula 85.
Just as we defined the <EM>k</EM>-th cohomology of <EM>E</EM>
as the quotient of the closed <EM>k</EM>-forms on <EM>E</EM>
by the exact <EM>k</EM>-forms, we can define the <EM>k</EM>-th
<EM>homology H<SUB>k</SUB></EM>(<EM>E</EM>) to be the quotient
of the closed <EM>k</EM>-chains in <EM>E</EM> by the boundaries
of (<EM>k</EM>+1)-chains, where as you might expect a chain is
said to be ``closed'' if its boundary is a trivial chain (this
generalizes the notion of a ``closed curve'').  For instance,
<EM>H</EM><SUB>0</SUB>(<EM>E</EM>) has one generator for each
component of <EM>E</EM>; when <EM>k</EM> is positive,
<EM>H<SUB>k</SUB></EM>(<EM>E</EM>) is trivial if <EM>E</EM> is convex;
and <EM>H</EM><SUB>1</SUB>(<STRONG>R</STRONG><SUP>2</SUP>-{0}) is
one-dimensional, generated by a circle around the origin.  Moreover,
thanks to generalized Stokes, integration yields a pairing of
<EM>H<SUB>k</SUB></EM>(<EM>E</EM>) with
<EM>H<SUP> k</SUP></EM>(<EM>E</EM>).  You will learn in
differential topology that this pairing often identifies each of these
two spaces with the other's dual.  You can already check that this
holds for the punctured plane <STRONG>R</STRONG><SUP>2</SUP>-{0}.

<P>

<img src="redball.gif">
The following remarks may help explain how come a function from
an open set in <STRONG>R</STRONG><SUP>3</SUP>
to <STRONG>R</STRONG><SUP>3</SUP> yields both a 1-form and a 2-form
(formulas 124, 125 on page 281).
In general, once we choose a generator for the <EM>n</EM>-th
exterior power of an <EM>n</EM>-dimensional vector space <EM>V</EM>
over some field (remember that this top exterior power is
1-dimensional), the exterior product yields for each <EM>k</EM>
a pairing between the <EM>k</EM>-th and (<EM>n-k</EM>)-th exterior
powers of <EM>V</EM>, which identifies each of the two spaces with
the other's dual.  In our case <EM>n</EM>=3 and <EM>k</EM>=1, so
we identify the dual of <STRONG>R</STRONG><SUP>3</SUP> with its
exterior square.  But, once we have chosen an inner product structure
on <STRONG>R</STRONG><SUP>3</SUP>, we have identified it with its
own dual; and once we also give <STRONG>R</STRONG><SUP>3</SUP> an
orientation, we get a generator for its top exterior power, namely
<EM>e</EM><SUB>1</SUB>^<EM>e</EM><SUB>2</SUB>^<EM>e</EM><SUB>3</SUB>
for any positively oriented orthonormal basis
(<EM>e</EM><SUB>1</SUB>, <EM>e</EM><SUB>2</SUB>,
<EM>e</EM><SUB>3</SUB>).
So, we have an identification of <STRONG>R</STRONG><SUP>3</SUP> with
its exterior square, and thus can naturally identify the 1- and 2-forms
on any open set in <STRONG>R</STRONG><SUP>3</SUP> with functions from
that set to <STRONG>R</STRONG><SUP>3</SUP>.

<blockquote>
At a more down-to-earth level, we can also use these ideas to explain
the ``cross product'' on an oriented 3-dimensional
real inner product space <em>V</em>:
the cross product <em>v</em>&nbsp;x&nbsp;<em>w</em>
is the image of <em>v</em>^<em>w</em>
under the map that identifies the exterior square of <em>V</em>
with <em>V</em>.
</blockquote>

A generator for the <EM>n</EM>-th exterior power
of an <EM>n</EM>-dimensional real vector space <EM>V</EM>
can be regarded as an <em>orientation</em> on <EM>V</EM>.
If <EM>V</EM> also carries an inner product structure,
then our construction identifies
differential <EM>k</EM>-forms and (<EM>n-k</EM>)-forms
on any open set in <em>V</em>; this identification
is known as the <em>Hodge star</em> operator from
<EM>k</EM>-forms to (<EM>n-k</EM>)-forms.
You can check that the Laplacian of a function <em>f</em>,
regarded as a 0-form, is just *<em>d</em>*<em>df</em>
-- which is one explanation of why the Laplacian is invariant
under orthogonal changes of coordinates.
For each <em>k</em>, *<em>d</em>*<em>d</em>
is an operator on the space of differential <em>k</em>-forms,
and may be regarded as the Laplacian on this space.
Can you write this operator explicitly as a differential operator on
(<em>n</em>!)<font size=+1>/</font>(<em>k</em>!(<em>n-k</em>)!)-tuples
of <em>C</em><sup>2</sup> functions?

<P>

<img src="redball.gif">
We're about to start on <STRONG>Fourier analysis</STRONG>, using
K&ouml;rner's book (a bit of this material can also be found in
Chapter 8 of Rudin).  The natural context for Fourier analysis
is the notion of a <STRONG>Hilbert space</STRONG>.  The one strange
choice K&ouml;rner makes is to suppress this notion entirely
(Hilbert's name does not even appear in the index!), as if to
prove that all of Fourier analysis can be done without Hilbert
spaces, as Axler set out to do all of linear algebra without
determinants.  But doing Fourier without Hilbert seems perverse.
We shall thus begin with an introduction to the structure of
Hilbert spaces, in particular separable ones.  The first handout
(<A HREF="hilbert1.pdf">hilbert1.pdf</A>,
<A HREF="hilbert1.ps">hilbert1.ps</A>)
gives the definition, basic examples,
and the fundamental notion of an orthonormal topological basis (ontb).
The second handout
(<A HREF="hilbert2.pdf">hilbert2.pdf</A>,
<A HREF="hilbert2.ps">hilbert2.ps</A>)
introduces separability, and the behavior of orthogonal complements,
projections, and duality for a Hilbert space.

<P>

<img src="redball.gif">
With what we know already, we can give another kind of proof that
the complex exponentials are an ontb for L<SUB>2</SUB>([0,1]):
their linear span is a <STRONG>C</STRONG>-subalgebra of
<EM>C</EM>([0,1]) which separates points and is ``self-adjoint''
(closed under complex conjugation); therefore, by the complex
Stone-Weierstrass theorem (Thm. 7.33 on page 165 of Rudin), it
is dense in <EM>C</EM>([0,1]) in the sup metric.  [In fact
Rudin's introduction to the theory of Fourier series in Chapter 8
uses this argument; see Thm. 8.15 on page 190.]
Thus <EM>a fortiori</EM> that linear span is dense in <EM>C</EM>([0,1])
also relative to the L<SUB>2</SUB> metric, and therefore
is dense in L<SUB>2</SUB>([0,1]), as desired.  Rudin only deals
with Fourier series in one dimension, but the same method also
works for Fourier series on <STRONG>T</STRONG><SUP><EM>n</EM></SUP>
for <EM>n</EM>>1.

<P>

<img src="redball.gif">
We'll proceed by developing Fourier analysis starting with
the following chapters of K&ouml;rner:
1 (Introduction),
2 (Statement and proof of Fej&eacute;r's theorem),
3 (Weyl equidistribution),
9,15,16,18 (some results on convergence and divergence of Fourier series).
Chapters 4 through 6 contain material most of which we have seen
already, but the second part of Chapter 6 may still surprise you.
You might also be interested in Chapter 19, which states without
proof several results much more precise (though necessarily much
harder) than is done in Chapter 18.

<P>

<img src="redball.gif">
Re Chapter 3: Weyl showed more generally the following theorem:
Let
<EM>t</EM><SUB>1</SUB>, <EM>t</EM><SUB>2</SUB>, <EM>t</EM><SUB>3</SUB>,...
be real numbers mod 2*pi, i.e., elements of <STRONG>T</STRONG>.  Then
the following are equivalent:
<UL>
<LI> For all <EM>a,b</EM> with 0 &lt; <EM>a</EM> &lt; <EM>b</EM> &lt; 1,
the number of <EM>t<SUB>r</SUB></EM>
with <EM>r</EM> &lt;= </EM>n</EM> lying in 2*pi[<EM>a,b</EM>]
is asymptotic to (<EM>b-a)</EM> <EM>n</EM>;
<LI> For any continuous function <EM>f</EM> from
<STRONG>T</STRONG> to <STRONG>C</STRONG>,
as <EM>n</EM> grows to infinity the average of
<EM>f</EM>(<EM>t<SUB>r</SUB></EM>) over <EM>r</EM> <= </EM>n</EM>
approaches the average of <EM>f</EM> over <STRONG>T</STRONG>;
<LI> For each nonzero integer <EM>s</EM>, the average of
<EM>e<SUP>ist<SUB>r</SUB></SUP></EM> approaches 0
as <EM>n</EM> grows to infinity.
</UL>
When these equivalent conditions hold, the sequence
{<EM>t<SUB>r</SUB></EM>} is said to be ``equidistributed mod 2*pi''.
Thus the proof of Thm. 3.1 is in effect the application of this
criterion to prove the equidistribution mod 1 of the sequence of
integer multiples of a given irrational.
Weyl's 1914 paper is reprinted in Vol.I (pages 487-497)
of his <em>Gesammelte Abhandlungen</em> (Collected Works),
which you can find in several of the Harvard libraries.

<P>

<img src="redball.gif">
Re Chapter 18: As noted in class, what's really going on here is the
following result: Let <EM>V,W</EM> be normed vector spaces, with
<EM>V</EM> complete.  Then any weakly convergent sequence
{<EM>T<SUB>n</SUB></EM>} in <EM>B</EM>(<EM>V,W</EM>) must be bounded;
in fact if {<EM>T<SUB>n</SUB></EM>} is unbounded then there are vectors
<EM>v</EM> in <EM>V</EM> such that |<EM>T<SUB>n</SUB>v</EM>|
approaches infinity.  (Recall that {<EM>T<SUB>n</SUB></EM>} is
said to <EM>converge weakly</EM> to a linear transformation <EM>T</EM>
if <EM>T<SUB>n</SUB>v</EM> approaches <EM>Tv</EM> for all <EM>v</EM> in
<EM>V</EM> -- in which case of course {<EM>T<SUB>n</SUB>v</EM>}
is bounded.)

<P>

To prove this result: Assume on the contrary that
{<EM>T<SUB>n</SUB></EM>} is unbounded.  Without loss of generality
(i.e. by passing to a subsequence), assume that
||<EM>T</EM><SUB>1</SUB>||>1 and, for each <EM>n</EM>, we have
||<EM>T<SUB>n</SUB></EM>|| > 100 ||<EM>T</EM><SUB><EM>n</EM>-1</SUB>||.
let <EM>v<SUB>n</SUB></EM> be a vector of norm 1 such that
|<EM>T<SUB>n</SUB>v<SUB>n</SUB></EM>| > ||<EM>T<SUB>n</SUB></EM>|| / 2.
Now define <EM>v</EM> as a sum of a convergent sequence thus:
<EM>u</EM><SUB>0</SUB>=0; given <EM>u<SUB>n</EM></SUB>,
choose for <EM>u</EM><SUB><EM>n</EM>+1</SUB></EM> whichever of
<EM>u<SUB>n</SUB></EM>+10<SUP><EM>-n</EM></SUP><EM>v<SUB>n</SUB></EM>
or
<EM>u<SUB>n</SUB></EM>-10<SUP><EM>-n</EM></SUP><EM>v<SUB>n</SUB></EM>
makes |<EM>T<SUB>n</SUB>u</EM><SUB><EM>n</EM>+1</SUB>| larger; by the
triangle inequality, |<EM>T<SUB>n</SUB>v</EM><SUB><EM>n</EM>+1</SUB>|
is then at least ||<EM>T<SUB>n</SUB></EM>||/(2*10<SUP><EM>n</EM></SUP>).
Let <EM>v</EM> be the limit of <EM>u<SUB>n</EM></SUB>.  Then
|<EM>T<SUB>n</SUB>v</EM>| is at least
(1/2-1/9)10<SUP><EM>-n</EM></SUP>||<EM>T<SUB>n</SUB></EM>||,
so approaches infinity as claimed.
[Except for the needlessly large factors of 100 and 10,
this is essentially what happens in Chapter 18 for the sequence
of functionals on <EM>C</EM>([0,1]) taking <EM>f</EM> to
<EM>S<SUB>n</SUB></EM>(<EM>f,t</EM>).]

<P>

<img src="redball.gif">
Next on the menu: Chapters 40 and 41, on orthogonal polynomials and
Gaussian <A HREF="#quadrature">quadrature</A>.  Unaccountably missing
from the end of Chapter 40 is the following result, which
I'll call ``Theorem 40.9'': let <EM>a,b,r,u<SUB>n</SUB></EM>
be as in Theorem 40.8; then there is a ``three-term recurrence''
<EM>u</EM><SUB><EM>n</EM>+1</SUB> =
(<EM>A<SUB>n</SUB>t</EM>+<EM>B<SUB>n</SUB></EM>)<EM>u<SUB>n</SUB></EM>
- <EM>C<SUB>n</SUB></EM><EM>u</EM><SUB><EM>n</EM>-1</SUB>,
for some real scalars
<EM>A<SUB>n</SUB> , B<SUB>n</SUB> , C<SUB>n</SUB></EM>.
For instance, the existence of such a recurrence for the
Tchebychev polynomials follows from the product formula for
cos(<EM>nx</EM>)cos(<EM>x</EM>).  In general, the recurrence
can be proved by comparing the expansions of (<EM>t u<SUB>n</SUB></EM>)
as a linear combination of the <EM>u<SUB>m</SUB></EM> and
(<EM>t u<SUB>m</SUB></EM>) as a linear combination of the
<EM>u<SUB>n</SUB></EM>.  This method also gives explicitly formulas
for <EM>A<SUB>n</SUB> , C<SUB>n</SUB></EM> (but not generally
<EM>B<SUB>n</SUB></EM>) in terms of the leading coefficients
and norms of the <EM>u<SUB>n</SUB></EM>.

<P>

All the results of Chapter 41 generalize directly to integration
with respect to an arbitrary weight on a finite interval, using
zeros of polynomials orthogonal with respect to that weight.
Note too that Lemma 41.1 can also be obtained as a consequence
of the fact that, over any field <EM>F</EM>, evaluation at
<EM>n</EM> distinct field elements is a linear bijection from
the polynomials of degree &lt;<EM>n</EM> to <EM>F<SUP>n</SUP></EM>.
This can be proved either computationally, as in K&ouml;rner,
or from general linear-algebra facts together with the observation
that the kernel of the map is zero.  The left-hand side (even with
an arbitrary weight function) is a linear functional on that space,
etc.

<P>

<img src="redball.gif">
We're starting on discrete Fourier analysis, from chapters 97ff.
in K&ouml;rner.  Fourier series, discrete Fourier analysis,
and the Fourier transform (which we'll proceed to next) are
special cases of <EM>Pontrjagin duality</EM>.  We shall not be
able to develop Pontrjagin duality in anything like its full scope
in 55b; even the basic theorems require detailed study of things like
Haar measure that are normally not established until well into a
graduate analysis course like Math 212.  But can still state some of
the definitions and key theorems to suggest the framework into which
the various Fourier variants fit.  Here's an
<A HREF="pontrjagin.html">outline</A>.
For another glimpse into the Pontrjagin framework,
in the special case of finite abelian groups where
everything can be stated and proved using only linear algebra,
see Chapters 103-4 of the textbook.

<P>

In each of our Fourier settings, to pointwise multiply two
functions on a group <EM>G</EM> we <EM>convolve</EM> their
Fourier transforms, and vice versa (up to scalar multiplication).
See Chapters 51, 52 for <EM>convolution</EM> on <STRONG>R</STRONG>
and <STRONG>T</STRONG>.  Convolution on 
<STRONG>Z</STRONG>/<EM>n</EM><STRONG>Z</STRONG>
is defined in 97.7 and described in 97.8 on page 495.
In general the convolution of two functions <EM>f,g</EM>
is the function <EM>f*g</EM> whose value at any <EM>t</EM>
is the sum/average/integral over <EM>x</EM> of
<EM>f</EM>(<EM>t-x</EM>)<EM>g</EM>(<EM>x</EM>).  We encountered
such things already in connection with the Dirichlet and
Fej&eacute;r kernels; they will also be the key to our use
of the fast Fourier transform (FFT, Chapter 98) to fast multiplication
(Chapter 99).  Note that the behavior of Pontrjagin duality on
subgroups is also an important ingerdient in the FFT, which
separates even and odd integers mod 2<EM>N</EM>.

<P>

<A NAME="today">
<img src="orangeball.gif">
Our final topic is <EM>Fourier transforms</EM>, Section IV of the
text.  We'll concentrate on the material in Chapters 46 (introduction,
and Fej&eacute;r part I), 49 (conclusion of Fej&eacute;r, and some
applications), 51 (convolution on <STRONG>R</STRONG>), and 60
(the inversion formula).  Chapters 47 and 48 contain warnings
and justifications of changing the order of integration that
should be familiar and routine by now; you may want to read them
for review purposes, but be aware that at least one typo is lurking
(In Lemma 48.6 on page 234, <EM>f</EM> is a function not on
<STRONG>R</STRONG> but on <STRONG>R</STRONG><SUP>2</SUP>).
For that matter there are at least two typos in Ch.46:
in Lemma 46.3 we need <EM>f</EM> to satisfy the hypotheses of Lemma 46.2
(integrability and absolute convergence), not 41.2;
and on the top line of p.224, it's the continuity of
<EM>f^</EM> that we need, not of <EM>f</EM>.
[Also, there's only one `m' in ``imitate''!]
The end of Chapter 49 recommends attention to Lemma 50.2, which
contains two important integral formulas.  Unfortunately
K&ouml;rner proves them with complex analysis, which most of you
probably don't know yet.  Fortunately 50.2(i) is also proved in Rudin
(Example 9.43, pages 237-8), and 50.2(ii) will be easy
once we have the inversion theorem of Chapter 60.
As a natural extension of the results of Chapter 51 we have
Parseval's identity and an application to the behavior at infinity
of Fourier transforms of integrable functions:
<A HREF="parseval.pdf">parseval.pdf</A>,
<A HREF="parseval.ps">parseval.ps</A>.

<HR>

<img src="redball.gif">
First problem set: Univariate differential calculus
(<A HREF="p1.ps">PS</A>, <A HREF="p1.pdf">PDF'</A>)
<strong>corrected</strong> 31.i.03 (see problem 5)

<br>

<img src="redball.gif">
Second problem set: Univariate integral calculus
(<A HREF="p2.ps">PS</A>, <A HREF="p2.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Third problem set: Univariate calculus cont'd, and Stone-Weierstrass
(<A HREF="p3.ps">PS</A>, <A HREF="p3.pdf">PDF'</A>)

<blockquote>
  <img src="purpball.gif">
  If Problems 6, 7, 8 are too easy or too familiar,
  see if you can evaluate the integral of
  cos<sup><em>n</em></sup>(<em>x</em>)&nbsp;cos(<em>cx</em>)
  from 0 to Pi/2, and deduce further product formulas.
</blockquote>

<br>

<img src="redball.gif">
Fourth problem set: Multivariate differential calculus
(<A HREF="p4.ps">PS</A>, <A HREF="p4.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Fifth problem set: Multivariate integral calculus
(<A HREF="p5.ps">PS</A>, <A HREF="p5.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Sixth problem set:
Interlude on convexity; introduction to differential forms
(<A HREF="p6.ps">PS</A>, <A HREF="p6.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Seventh problem set:
Differential forms, chains, integration, and more exterior algebra
(<A HREF="p7.ps">PS</A>, <A HREF="p7.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Eighth problem set:
Life in Hilbert space
(<A HREF="p8.ps">PS</A>, <A HREF="p8.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Ninth problem set:
Fourier series I
[summability methods, Weyl equidistribution, and differentiation]
(<A HREF="p9.ps">PS</A>, <A HREF="p9.pdf">PDF'</A>)

<br>

<img src="redball.gif">
Tenth problem set:
Fourier series, cont'd; orthogonal polynomials
(<A HREF="p10.ps">PS</A>, <A HREF="p10.pdf">PDF'</A>)

<br>

<A NAME="current_homework">
<img src="orangeball.gif">
Eleventh and last problem set:
Fourier series, the grand finale: Gauss, Jacobi, Poisson, and M&uuml;ntz
(<A HREF="p11.ps">PS</A>, <A HREF="p11.pdf">PDF'</A>)

<P>

<img src="http:/~elkies/greenball.gif">
<A HREF="pp.pdf">pp.pdf</A>, <A HREF="pp.ps">pp.ps</A>:
An assortment of practice problems
