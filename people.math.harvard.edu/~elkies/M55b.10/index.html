<TITLE>
Math 55b: Honors Real and Complex Analysis
</TITLE>

<BODY BACKGROUND="calabi1.gif">

<strong>
Lecture notes, etc., for
Math 55b: Honors Real and Complex Analysis
(Spring [2010-]2011)
</strong>

<P>

If you find a mistake, omission, etc., please
<A HREF="mailto:elkies@math.harvard.edu">let me know</A>
by e-mail.

<P>

The <FONT COLOR="#B87800">orange</FONT COLOR> balls
mark our <A HREF="#today">current location</A> in the course,
and the <A HREF="#current_homework">current problem set</A>.

<hr>

<img src="redball.gif">
My office hours are the same as for Math 55a:
Fridays 3:00&ndash;4:30 PM at my office (Sci Ctr 335),
or by appointment
<br>
<img src="redball.gif">
Tony Feng's section and office hours:
<br>
Section: Thursdays 4&ndash;5 PM, Science Center 310
<br>
Office hours: Thursdays 8&ndash;9 PM, also in Science Center 310

<HR>

<img src="redball.gif">
Our first topic is the <em>topology of metric spaces</em>,
a fundamental tool of modern mathematics
that we shall use mainly as a key ingredient in our rigorous development
of differential and integral calculus
over&nbsp;<strong>R</strong> and&nbsp;<strong>C</strong>.
To supplement the treatment in Rudin's textbook,
I wrote up 20-odd pages of notes in six sections;
copies will be distributed in class, and you also may view them
and print out copies in advance from the PDF files linked below.
<font size="-1">[Some of the explanations, as of notations such as
<nobr><em>f</em>&thinsp;(&middot;)</nobr> and the triangle inequality
in&nbsp;<strong>C</strong>, will not be necessary; they were needed
when this material was the initial topic of Math&nbsp;55<u><em>a</em></u>,
and it doesn't feel worth the effort to delete them now that it's been
moved to&nbsp;55b.  Likewise for the comment about the Euclidean distance
at the top of page&nbsp;2 of the initial handout on
&ldquo;basic definitions and examples&rdquo;.]</font>

<P>

<img src="redball.gif">
<A HREF="top1.pdf" target="_blank">Metric Topology I</a>
<BR>
Basic definitions and examples:
the metric spaces <strong>R</strong><sup><em>n</em></sup>
and other product spaces; isometries; boundedness and function spaces

<blockquote>
  <img src="purpball.gif">
  The &ldquo;sup metric&rdquo; on <em>X<sup>S</sup></em>
  is sometimes also called the &ldquo;uniform metric&rdquo; because
  <nobr><em>d</em>(<em>f</em>,&thinsp;<em>g</em>)&le;<em>r</em></nobr>
  is equivalent to a bound
  <nobr><em>d</em>(<em>f</em>(<em>s</em>),&thinsp;<em>g</em>(<em>s</em>))&le;<em>r</em></nobr>
  for all <nobr><em>s</em> in <em>S</em></nobr> that is &ldquo;uniform&rdquo;
  in the sense that it's independent of the choice of&nbsp;<em>s</em>.
  Likewise for the sup&nbsp;metric on the space of bounded functions
  from&nbsp;<em>S</em> to an arbitrary metric space&nbsp;<em>X</em>
  (see the next paragraph).
</blockquote>

<blockquote>
  <img src="purpball.gif">
  If <em>S</em> is an infinite set and
  <em>X</em> is an unbounded metric space then
  we can't use our definition of <em>X</em><sup><em>S</em></sup>
  as a metric space because
 <nobr>sup<sub><em>S</em></sub>&nbsp;<em>d<sub>X</sub></em>(<em>f</em>(<em>s</em>),&thinsp;<em>g</em>(<em>s</em>))</nobr>
  might be infinite.  But the <em>bounded functions</em> from <em>S</em>
  to <em>X</em> <u>do</u> constitute a metric space under the same
  definition of <em>d<sub>X<sup>S</sup></sub></em>.  A function
  is said to be &ldquo;bounded&rdquo; if its image is a bounded set.
  You should check that
  <nobr><em>d<sub>X<sup>S</sup></sub></em>(<em>f</em>,&thinsp;<em>g</em>)</nobr>
  is in fact finite for bounded <em>f</em> and <em>g</em>.
</blockquote>

<blockquote>
  <img src="purpball.gif">
  Now that metric topology is in 55b, not 55a, the following 
  observation can be made: if <em>X</em> is <strong>R</strong>
  or <strong>C</strong>, the bounded functions in
  <em>X</em><sup><em>S</em></sup> constitute a vector space,
  and the sup metric comes from a norm on that vector space:
  <nobr><em>d</em>(<em>f</em>,&thinsp;<em>g</em>) =
   ||<em>f</em>&minus;<em>g</em>||</nobr>
  where the norm <nobr>||&middot;||</nobr> is defined by
  <nobr>||<em>f</em>&thinsp;|| = sup<sub>s</sub> |<em>f</em>(<em>s</em>)|</nobr>.
  Likewise for the bounded functions from <em>S</em> to any normed
  vector space.  Such spaces will figure in our development of real
  analysis (and in your further study of analysis beyond Math&nbsp;55).
</blockquote>

<blockquote>
  <img src="purpball.gif">
  The &ldquo;Proposition&rdquo; on page 3 of the first topology handout
  can be extended as follows:
  <blockquote>
   iv) For <strong>every</strong> point <em>p</em> of <em>X</em>
   there exists a real number <em>M</em> such that
   </nobr><em>d</em>(<em>p</em>,<em>q</em>) &lt; <em>M</em></nobr>
   for all <em>q</em> of <em>E</em>.
  </blockquote>
  In other words, for every <em>p</em> in <em>X</em>
  there exists an open ball about <em>p</em> that contains <em>E</em>.
  Do you see why this is equivalent to (i), (ii), and (iii)?
</blockquote>

<img src="redball.gif">
<A HREF="top2.pdf" target="_blank">Metric Topology II</a>
<BR>
Open and closed sets, and related notions
<br>
<strong>tweaked</strong> 27.i.11 to correct typo
&ldquo;limits points&rdquo; at the bottom of page 3, remove
superfluous parentheses around &ldquo;sup&nbsp;<em>E</em>&rdquo;
near the end of page&nbsp;4 (twice), and make overlines in
<span style="text-decoration: overline">B</span>,
<span style="text-decoration: overline">E</span>, etc.
look nicer

<P>

<img src="redball.gif">
<A HREF="top3.pdf" target="_blank">Metric Topology III</a>
<BR>
Introduction to functions and continuity
<br>
<strong>tweaked</strong> 27.i.11: the Appendix gives
<em>a</em> proof that (<strong>C</strong>, <em>d</em>)
is a metric space, but probably not
<em>the</em> [only] proof&hellip;
<br>
<strong>tweaked</strong> 30.i.11 to correct a minor typo on page&nbsp;2:
at the end of the proof of the Theorem (Rudin&nbsp;4.8), the
<nobr>&epsilon;-neighborhood</nobr> is of course
<nobr><em>N</em><sub>&epsilon;</sub>(&thinsp;<em>f</em>&thinsp;(<em>p</em>))</nobr>,
not <nobr><em>N<sub>r</sub></em>(&thinsp;<em>f</em>&thinsp;(<em>p</em>))</nobr>.


<P>

<img src="redball.gif">
<A HREF="top4.pdf" target="_blank">Metric Topology IV</a>
<BR>
Sequences and convergence, etc.
<BR>
<strong>tweaked</strong> 29.i.11 for overlines (see 27.i.11 tweak above)

<P>

<img src="redball.gif">
<A HREF="top5.pdf" target="_blank">Metric Topology V</a>
<BR>
Compactness and sequential compactness
<br>
<strong>tweaked</strong> 29.i.11 for overlines (see 27.i.11 tweak above)

<P>
<img src="redball.gif">
<A HREF="top6.pdf" target="_blank">Metric Topology VI</a>
<BR>
Cauchy sequences and related notions
(completeness, completions, and a third formulation of compactness)
<br>
<strong>tweaked</strong> 9.ii.11 (reference in p.3 is to
Problem 4 of PS1, not Problem 5)
<blockquote>
  <img src="purpball.gif"> Here is a more direct proof of the theorem that
  a continuous map
  <nobr><em>f</em> : <em>X</em> &rarr; <em>Y</em></nobr>
  between metric spaces is uniformly continuous if <em>X</em>
  is compact.
  Assume not.  Then there exists
  <nobr>&epsilon;&thinsp;&gt;&thinsp;0</nobr>
  such that for all <nobr>&delta;&thinsp;&gt;&thinsp;0</nobr>
  there are some points
  <nobr><em>p</em>, <em>q</em></nobr> in&nbsp;<em>X</em>
  such that
  <nobr><em>d</em>(<em>p</em>, <em>q</em>) &lt; &delta;</nobr>
  but
  <nobr><em>d</em>(<em>f</em>&thinsp;(<em>p</em>), <em>f</em>&thinsp;(<em>q</em>)) &ge; &epsilon;.</nobr>
  For each <nobr><em>n</em> = 1, 2, 3, &hellip;</nobr>,
  choose <nobr><em>p<sub>n</sub></em>, <em>q<sub>n</sub></em></nobr>
  that satisfy those inequalities for <nobr>&delta; = 1/<em>n</em>.</nobr>
  Since <em>X</em> is assumed (sequentially) compact, we can extract
  a subsequence <nobr>{<em>p<sub>n<sub>i</sub></sub></em>}</nobr>
  of <nobr>{<em>p<sub>n</sub></em>}</nobr>
  that converges to some <em>p</em> in&nbsp;<em>X</em>.
  But then <nobr>{<em>q<sub>n<sub>i</sub></sub></em>}</nobr>
  converges to the same&nbsp;<em>p</em>.  Hence both
  <nobr><em>f</em>&thinsp;(<em>p<sub>n<sub>i</sub></sub></em>)</nobr>
  and <nobr><em>f</em>&thinsp;(<em>q<sub>n<sub>i</sub></sub></em>)</nobr>
  converge to <nobr><em>f</em>&thinsp;(<em>p</em>)</nobr>,
  which contradicts the fact that
  <nobr><em>d</em>(<em>f</em>&thinsp;(<em>p<sub>n<sub>i</sub></sub></em>), <em>f</em>&thinsp;(<em>q<sub>n<sub>i</sub></sub></em>)) &ge; &epsilon;</nobr>
  for each&nbsp;<em>i</em>.
</blockquote>

<hr>

<img src="redball.gif">
Our next topic is <em>differential calculus</em> of vector-valued
functions of one real variable, building on Chapter 5 of Rudin.

<blockquote>
You may have already seen &ldquo;little oh&rdquo; and &ldquo;big Oh&rdquo; notations.
For functions <nobr><em>f</em>, <em>g</em></nobr> on the same space,
<nobr>&ldquo;<em>f</em> = <em>O</em>(<em>g</em>)&rdquo;</nobr> means that
&thinsp;<em>g</em>&thinsp; is a nonnegative real-valued function,
<em>f</em> takes values in a normed vector space,
and there exists a real constant <em>M</em> such that
<nobr>|<em>f</em>(<em>x</em>)|&le;<em>Mg</em>(<em>x</em>)</nobr>
for all <em>x</em>.  The notation
<nobr>&ldquo;<em>f</em> = <em>o</em>(<em>g</em>)&rdquo;</nobr>
is used in connection with a limit; for instance,
<nobr>&ldquo;<em>f</em>(<em>x</em>) = <em>o</em>(<em>g</em>(<em>x</em>))</nobr>
as <em>x</em> approaches <em>x</em><sub>0</sub>&rdquo; indicates that
<nobr><em>f</em>, <em>g</em></nobr> are vector- and real-valued
 functions as above on some neighborhood of&nbsp;<em>x</em><sub>0</sub>,
and that for each <nobr>&epsilon;&gt;0</nobr> there is a neighborhood
 of <em>x</em><sub>0</sub> such that
<nobr> |<em>f</em>(<em>x</em>)|&le;&epsilon;<em>g</em>(<em>x</em>)</nobr>
for all <em>x</em> in the neighborhood.  Thus
</nobr><em>f</em>&thinsp;'(<em>x</em><sub>0</sub>) = <em>a</em></nobr>
 means the same as
<nobr>&ldquo;<em>f</em>&thinsp;(<em>x</em>) = <em>f</em>&thinsp;(<em>x</em><sub>0</sub>) + <em>a</em>(<em>x</em>&minus;<em>x</em><sub>0</sub>) + <em>o</em>(|<em>x</em>&minus;<em>x</em><sub>0</sub>|)</nobr>
as <em>x</em> approaches <em>x</em><sub>0</sub>&rdquo;,
with no need to exclude the case <em>x</em> = <em>x</em><sub>0</sub>.
Rudin in effect uses this approach when proving the Chain Rule (5.5).
<p>
Apropos the Chain Rule: as far as I can see we don't need continuity
of&nbsp;<em>f</em>&thinsp; at any point except&nbsp;<em>x</em>
(though that hypothesis will usually hold in any application).
All that's needed is that <em>x</em> has some relative neighborhood
<em>N</em> <nobr> in [<em>a</em>,<em>b</em>]</nobr> such that
<nobr><em>f</em>&thinsp;(<em>N</em>)</nobr> is contained
in&nbsp;<em>I</em>.  Also, it is necessary that <em>f</em>&thinsp;
map <nobr>[<em>a</em>,<em>b</em>]</nobr> to&nbsp;<strong>R</strong>,
but <em>g</em> can take values in any normed vector space.
<p>
The derivative of <em>f</em>&thinsp;/<em>g</em>
can be obtained from the product rule,
together with the derivative of <nobr>1/<em>g</em></nobr>
&mdash; which in turn can be obtained from the Chain Rule together
with the the derivative of the single function 1/<em>x</em>.
Once we do multivariate differential calculus,
we'll see that the derivatives of
  <em>f</em>&thinsp;+<em>g</em>, <em>f</em>&thinsp;&minus;<em>g</em>,
  <em>f</em><em>g</em>, <em>f</em>&thinsp;/<em>g</em>
could also be obtained in much the same way
that we showed the continuity of those functions,
by combining the multivariate Chain Rule
with the derivatives of the specific functions
  <em>x</em>+<em>y</em>, <em>x</em>&minus;<em>y</em>,
  <em>x</em><em>y</em>, <em>x</em>/<em>y</em>
of two variables <em>x</em>,<em>y</em>.
<p>
As Rudin notes at the end of this chapter, differentiation can also
be defined for vector-valued functions of one real variable.  As Rudin
does <u>not</u> note, the vector space can even be infinite-dimensional,
provided that it is normed; and the basic algebraic properties of the
derivative listed in Thm. 5.3 (p.104) can be adapted to this generality,
e.g., the formula
<nobr>(<em>fg</em>)' = <em>f</em>&thinsp;'<em>g</em> + <em>fg'</em></nobr>
still holds if <em>f</em>,&thinsp;<em>g</em>
take values in normed vector spaces <em>U</em>,&thinsp;<em>V</em>
and multiplication is interpreted as a continuous bilinear map from
<nobr><em>U</em> &times; <em>V</em></nobr>
to some other normed vector space&nbsp;<em>W</em>.
<p>
Rolle's Theorem is the special case
<nobr><em>f</em>&thinsp;(<em>b</em>) = <em>f</em>&thinsp;(<em>a</em>)</nobr>
of Rudin's Theorem 5.10; as you can see it is in effect the key step
in his proof of Theorem 5.9, and thus of 5.10 as well.
<p>
We omit 5.12 (continuity of derivatives) and 5.13 (L'H&ocirc;pital's Rule).
In 5.12, see p.94 for Rudin's notion of &ldquo;simple discontinuity&rdquo;
(or &ldquo;discontinuity of the first kind&rdquo;) vs.
&ldquo;discontinuity of the second kind&rdquo;, but please don't
use those terms in your problem sets or other mathematical writing,
since they're not widely known.
In Rudin's proof of L'H&ocirc;pital's Rule (5.13),
why can he assume that <nobr><em>g</em>(<em>x</em>)</nobr>
does not vanish for any <em>x</em> in <nobr>(<em>a</em>,<em>b</em>)</nobr>,
and that the denominator
 <nobr><em>g</em>(<em>x</em>)&minus;<em>g</em>(<em>y</em>)</nobr>
in equation (18) is never zero?

<p>

NB The norm does not have to come from an inner product structure.
Often this does not matter because we work in finite dimensional
vector spaces, where all norms are equivalent, and changing to
an equivalent norm does not affect the definition of the derivative.
The one exception to this is Thm. 5.19 (p.113) where one needs the
norm exactly rather than up to a constant factor.  This theorem still
holds for a general norm but requires an additional argument.
<A NAME="HahnBanach">
The key ingredient of the proof is this: given a nonzero vector
<em>z</em> in a vector space <em>V</em>, we want a continuous
functional <em>w</em> on <em>V</em> such that
</nobr>||<em>w</em>|| = 1</nobr> and <nobr><em>w(z)</em> = |<em>z</em>|.</nobr>
If <em>V</em> is an inner product space (finite-dimensional or not),
the inner product with <nobr><em>z</em>&thinsp;/&thinsp;|<em>z</em>|</nobr>
provides such a functional <em>w</em>.
But this approach does not work in general.  The existence of
such <em>w</em> is usually proved as a corollary of the Hahn-Banach
theorem.  When <em>V</em> is finite dimensional, <em>w</em> can
be constructed by induction on the dimension of <em>V</em>.
To deal with the general case one must also invoke the Axiom of Choice
in its usual guise of Zorn's Lemma.

</blockquote>

<img src="redball.gif">
We next start on <strong>univariate integral calculus</strong>,
largely following Rudin, chapter 6.
The following gives some motivation for the definitions there.
(And yes, it's the same Riemann who gave number theorists like me
the Riemann zeta function and the Riemann Hypothesis.)

<blockquote>
The Riemann-sum approach to integration goes back to the
&ldquo;method of exhaustion&rdquo; of classical Greek geometry,
in which the area of a plane figure (or the volume of a region in space)
is bounded below and above by finding subsets and supersets
that are finite unions of disjoint rectangles (or boxes).
The lower and upper Riemann sums adapt this idea
to the integrals of functions which may be negative as well as positive
(recall that one of the weaknesses of geometric Greek mathematics is that
the ancient Greeks had no concept of negative quantities
&mdash; nor, for that matter, of zero).
<A NAME="quadrature">
You may have encountered the quaint technical term &ldquo;quadrature&rdquo;,
used in some contexts as a synonym for &ldquo;integration&rdquo;.
This too is an echo of the geometrical origins of integration.
&ldquo;Quadrature&rdquo; literally means &ldquo;squaring&rdquo;, meaning not
&ldquo;multiplying by itself&rdquo; but &ldquo;constructing a square of
the same size as&rdquo;; this in turn is equivalent to
&ldquo;finding the area of&rdquo;,
as in the phrase &ldquo;squaring the circle&rdquo;.
For instance, Greek geometry contains a theorem equivalent
to the integration of <nobr><em>x</em><sup>2</sup><em>dx</em></nobr>,
a result called the &ldquo;quadrature of the parabola&rdquo;.
The proof is tantamount to the evaluation of lower and upper Riemann sums
 for the integral <nobr>of <em>x</em><sup>2</sup><em>dx</em></nobr>.

<p>

An alternative explanation of the upper and lower Riemann sums,
and of &ldquo;partitions&rdquo; and &ldquo;refinements&rdquo;
(Definitions 6.1 and 6.3 in Rudin),
is that they arise by repeated application of the following two
axioms describing the integral (see for instance
<a href="gillman.pdf" target="_blank">L.Gillman's expository paper</a>
in the <em>American Mathematical Monthly</em> (Vol.100&nbsp;#1, 16&ndash;25)):
<ul>
<li> For any <em>a,b,c</em> (with <em>a &lt; b &lt; c</em>),
the integral of a function from <em>a</em> to <em>c</em>
is the sum of its integrals
from <em>a</em> to&nbsp;<em>b</em> and from <em>b</em> to&nbsp;<em>c</em>;
<li> If a function <em>f</em> on [<em>a,b</em>] takes values in
[<em>m,M</em>] then its integral from <em>a</em> to <em>b</em>
is in
<nobr>[<em>m</em>(<em>b&minus;a</em>), <em>M</em>(<em>b&minus;a</em>)]</nobr>
(again assuming <nobr><em>a &lt; b</em>).</nobr>
</ul>
The latter axiom is a consequence of the following two: the integral of
a constant function from <em>a</em> to <em>b</em> is that constant
times the length <nobr><em>b&minus;a</em></nobr>
of the interval <nobr>[<em>a,b</em>];</nobr>
and if <nobr><em>f</em> &le; <em>g</em></nobr> on some interval
then the integral of <em>f</em> over that interval
does not exceed the integral of&nbsp;<em>g</em>.
Note that again all these axioms arise naturally from an interpretation
of the integral as a &ldquo;signed area&rdquo;.

<p>

The (Riemann-)Stieltjes integral,
with <em>d</em>&alpha; in place of <em>dx</em>,
is then obtained by replacing each
<nobr>&Delta;<em>x</em> = <em>b&minus;a</em></nobr> by
<nobr>&Delta;&alpha; = &alpha;(<em>b</em>)&minus;&alpha;(<em>a</em>)</nobr>.

<p>

In Theorem 6.12, property (a) says the integrable functions form
a vector space, and the integral is a linear transformation;
property (d) says it's a bounded transformation relative to the
sup norm, with operator norm at most
<nobr>&Delta;&alpha; = &alpha;(<em>b</em>)&minus;&alpha;(<em>a</em>)</nobr>
(indeed it's not hard to show that the operator norm equals
<nobr>&Delta;&alpha; = &alpha;(<em>b</em>)&minus;&alpha;(<em>a</em>)</nobr>);
and (b) and (c) are the axioms noted above.  Property (e) <em>almost</em>
says the integral is linear as a function of&nbsp;&alpha; &mdash;
do you see why &ldquo;almost&rdquo;?

<p>

Recall the &ldquo;integration by parts&rdquo; identity: <em>fg</em>
is an integral of <nobr><em>f dg</em> + <em>g df</em></nobr>.
The Stieltjes integral
is a way of making sense of this identity even when <em>f</em>
and/or <em>g</em> is not continuously differentiable.  To be sure,
some hypotheses on <em>f</em> and <em>g</em> must still
be made for the Stieltjes integral of <em>f dg</em> to make sense.
Rudin specifies one suitable system of such hypotheses in Theorem 6.22.

<P>

<A HREF="vint.pdf">Here</A>'s a version of Riemann-Stieltjes integrals
that works cleanly for integrating bounded functions from
<nobr>[<em>a</em>,<em>b</em>]</nobr> to any complete normed vector space.
<br>
<img src="purpball.gif">
<strong>corrected</strong> 27.ii.11 to fix a minor typo
(&ldquo;reasonable hypothes<u><em>e</em></u>s&rdquo;,
not &ldquo;reasonable hypothes<u><em>i</em></u>s&rdquo;).

<p>

<em>Riemann-Stieltjes integration by parts</em>: Suppose both
<em>f</em> and <em>g</em> are increasing functions on
[<em>a,b</em>].  For any partition
<nobr><em>a</em> = <em>x</em><sub>0</sub></em> &lt; &hellip; &lt; <em>x<sub>n</sub></em> = <em>b</em></nobr>
of the interval, write
<em>f</em>(<em>b</em>)<em>g</em>(<em>b</em>)
&minus; <em>f</em>(<em>a</em>)<em>g</em>(<em>a</em>)
as the telescoping sum of
<em>f</em>(<em>x<sub>i</sub></em>)<em>g</em>(<em>x<sub>i</sub></em>) &minus;
<em>f</em>(<em>x</em><sub><em>i</em>&minus;1</sub>)<em>g</em>(<em>x</em><sub><em>i</em
>&minus;1</sub>)
from <em>i</em>=1 to <em>n</em>.  Now rewrite the <em>i</em>-th summand as
<center>
<em>f</em>&thinsp;(<em>x<sub>i</sub></em>)
(<em>g</em>(<em>x<sub>i</sub></em>)&minus;<em>g</em
>(<em>x</em><sub><em>i</em>&minus;1</sub>)) +
<em>g</em>(<em>x</em><sub><em>i</em>&minus;1</sub>)
(<em>f</em>&thinsp;(<em>x<sub>i</sub></em>
)&minus;<em>f</em>&thinsp;(<em>x</em><sub><em>i</em>&minus;1</sub>)).
</center>
[Naturally it is no accident that this identity resembles the
one used in the familiar proof of the formula for the derivative of
<em>fg</em>&nbsp;!]  Summing this over <em>i</em> yields the upper
Riemann-Stieltjes sum for the integral of <em>f dg</em> plus
the lower R.-S. sum for the integral of <em>g df</em>.  Therefore:
if one of these integrals exists, so does the other, and their sum is
<nobr><em>f</em>&thinsp;(<em>b</em>)<em>g</em>(<em>b</em>)
&minus; <em>f</em>&thinsp;(<em>a</em>)<em>g</em>(<em>a</em>)</nobr>.
[Cf. Rudin, page 141, Exercise 17.]

</blockquote>

<img src="redball.gif">
Most of <strong>Chapter 7</strong> of Rudin we've covered already
in the topology lectures and problem sets.  For more counterexamples
along the lines of the first section of that chapter, see
<em>Counterexamples in Analysis</em> by B.R.Gelbaum and J.M.H.Olsted
&mdash; there are two copies in the Science Center library (QA300.G4).
Concerning Thm. 7.16, be warned that it can easily fail for
&ldquo;improper integrals&rdquo; on infinite intervals
(see Rudin, p.138, Exercise 8, assigned on PS2).
It is often very useful
to bring a limit or an infinite sum within an integral sign,
but this procedure requires justification beyond Thm.&nbsp;7.16.

<p>

We'll cover most of the new parts of Chapter&nbsp;7:
<ul>
<li>One counterexample, 7.4;
<li>Weierstrass M, 7.10, extended to
vector-valued functions;
<li> uniform convergence and &int;&nbsp;&nbsp;(7.16,
  again in vector-valued setting, with the target space <em>V</em>
  normed and complete);
<li>uniform convergence and <nobr><em>d</em>/<em>dx</em></nobr>
  (7.17, also taking values in a complete normed vector
  space&nbsp;<em>V</em>, which for us is either finite-dimensional or
  an inner-product space since we're not proving Hahn-Banach); and
<li>another counterexample, 7.18.
</ul>
We'll then outline the
<strong>Stone-Weierstrass theorem</strong>,
which is the one major result of Chapter&nbsp;7 we haven't seen yet.
We then proceed to <strong>power series</strong> and the exponential
and logarithmic functions in <strong>Chapter 8</strong>.
We omit most of the discussion of Fourier series (185&ndash;192),
an important topic (which used to be what I concluded Math&nbsp;55b with),
but one that alas cannot be accommodated given the mandates of
the curricular review.  We'll encounter a significant special case
in the guise of Laurent expansions of an analytic function on a disc.
See these notes (<a href="hilbert1.pdf" target="_blank">part&nbsp;1</A>,
<a href="hilbert2.pdf" target="_blank">part&nbsp;2</A>) from 2002-3 on
<strong>Hilbert space</strong> for a fundamental context for Fourier
series and much else (notably much of quantum mechanics),
which is also what we'll use to give one proof of M&uuml;ntz's theorem
on uniform approximation by arbitrary powers.

<p>

We also postpone discussion of Euler's Beta and Gamma integrals
(also in Chapter&nbsp;8) so that we can use multivariate integration to
give a more direct proof of the formula relating them.

<P>

The result concerning the convergence of alternating series
is stated and proved on pages <nobr>70-71</nobr> of Rudin (Theorem 3.42).

<P>

The original Weierstrass approximation theorem (7.26 in Rudin)
can be reduced to the uniform approximation of the single function
<nobr>|<em>x</em>|</nobr> on <nobr>[&minus;1,1]</nobr>.
From this function we can construct
an arbirtrary piecewise linear continuous function, and such
piecewise linear functions uniformly approximate any continuous function
on a closed interval.  To get at </nobe>|<em>x</em>|</nobr>,
we'll rewrite it as
<nobr>[1&minus;(1&minus;<em>x</em><sup>2</sup>)]<sup>1/2</sup></nobr>,
and use the power series for <nobr>(1&minus;<em>X</em>)<sup>1/2</sup></nobr>.
This power series (and more generally the power series for
<nobr>(1&minus;<em>X</em>)<sup><em>A</em></sup></nobr>)
is the first part of Exercise&nbsp;22 for Chapter&nbsp;8, on p.201;
we've already outlined another approach in
<a href="p5.pdf" target="_blank">PS5</a>, Problem&nbsp;3
(under the assumption of the standard formula for differentiating
<nobr><em>x<sup>r</sup></em></nobr> with respect to&nbsp;<em>x</em>,
which as we note there is not too hard for <em>r</em> rational).
We need <nobr>(1&minus;<em>x</em>)<sup>1/2</sup></nobr>
to be approximated by its power series uniformly on the
<em>closed</em> interval <nobr>[&minus;1,1]</nobr> (or at least [0,1]);
but fortunately this too follows from the proof
of Abel's theorem (8.2, pages <nobr>174-5</nobr>).  Actually this is a subtler
result than we need, since the <nobr><em>X<sup>n</sup></em></nobr> coefficient
of the power series for <nobr>(1-<em>X</em>)<sup>1/2</sup></nobr> is negative
for every <nobr><em>n</em>&gt;0</nobr>.  If a power series in <em>X</em>
has radius of convergence&nbsp;1 and all but finitely many of its nonzero
coefficients have the same sign, then it is easily shown that the sum of
the coefficients converges if and only if <nobr><em>f</em>(<em>X</em>)</nobr>
has a finite limit as <em>X</em> approachess&nbsp;1, in which case the sum
equals that limit and the power series converges uniformly on
<nobr>[0,1]</nobr>.
That's all we need because clearly <nobr>(1-<em>X</em>)<sup>1/2</sup></nobr>
extends to a continuous function on <nobr>[0,1]</nobr>.
(For an alternative approach to uniformly approximating
<nobr>|<em>x</em>|<nobr>, see exercise 23 on <nobr>p.169.)</nobr>

<p>

Rudin's notion of an &ldquo;algebra&rdquo; of functions is <em>almost</em>
a special case of what we called an 
&ldquo;algebra over&nbsp;<strong>F</strong>&rdquo; in 55a
(with <nobr><strong>F</strong> = <strong>R</strong></nobr>
or&nbsp;<strong>C</strong> as usual), except that Rudin
does not require his algebras to have a unit (else he wouldn't
have to impose the &ldquo;vanish on no point&rdquo; condition).
The notion can be usefully abstracted to a
&ldquo;normed algebra over&nbsp;<strong>F</strong>&rdquo;,
which is an algebra together with a vector space norm
<nobr>||&middot;||</nobr> satisfying
<nobr>||<em>xy</em>|| &le; ||<em>x</em>|| ||<em>y</em>||</nobr>
for all <em>x</em> and <em>y</em> in the algebra.
Among other things this leads to the Stone-&#x010c;ech theorem.

<p>

<img src="redball.gif">
In the first theorem of <strong>Chapter 8</strong>,
Rudin obtains the termwise differentiability of a power series at any
<nobr>|<em>x</em>| &lt; <em>R</em></nobr>
by applying Theorem&nbsp;7.17.  That's nice, but we'll want to use
the same result in other contexts, notably over&nbsp;<strong>C</strong>,
where the mean value theorem does not apply.  So we'll instead give
an argument that works in any complete field with an absolute value
&mdash; this includes <strong>R</strong>, <strong>C</strong>,
and other examples such as the field
<nobr><strong>Q</strong><sub><em>p</em></sub></nobr>
of <nobr><em>p</em>-adic</nobr> numbers.  If the sum of
<nobr><em>c<sub>n</sub> x<sup>n</sup></em></nobr>
converges for some nonzero <em>x</em> with
<nobr>|<em>x</em>| = <em>R</em></nobr>, then any <em>x</em>
satisfying <nobr>|<em>x</em>| &lt; <em>R</em></nobr> has
a neighborhood that is still contained in
<nobr>{<em>y</em> : |<em>y</em>| &lt; <em>R</em>}</nobr>.
So if <nobr><em>f</em>&thinsp;(<em>x</em>)</nobr> is the sum of
that series, then for <nobr><em>y</em> &ne; <em>x</em></nobr>
in that neighborhood we may form the usual quotient
<nobr>(<em>f</em>&thinsp;(<em>x</em>)&minus;<em>f</em>&thinsp;(<em>y</em>))
/ (<em>x&minus;y</em>)</nobr> and expand it termwise, then let
<nobr><em>y</em>&rarr;<em>x</em></nobr> and recover the expected
power series for <nobr><em>f</em>&thinsp;'(<em>x</em>)</nobr>
using the Weierstrass <em>M</em> test (Theorem 7.10).

<p>

An alternative derivation of formula (26) on p.179:
differentiate the power series (25) termwise (now that we know
it works also over&nbsp;<strong>C</strong>) to show
<nobr><em>E</em>(<em>z</em>) = <em>dE</em>(<em>z</em>)/<em>dz</em></nobr>;
then for any fixed <em>w</em> the difference
<nobr><em>E</em>(<em>w</em>+<em>z</em>) 
&minus; <em>E</em>(<em>w</em>) <em>E</em>(<em>z</em>)</nobr>
is an <em>analytic</em> function of&nbsp;<em>z</em> that vanishes
at <nobr><em>z</em> = 0</nobr> and is thus zero everywhere.

<p>

<strong>Error in Rudin</strong>: the argument on p.180 that
&ldquo;Since <em>E</em> is strictly increasing and differentiable on
[the real numbers], it has an inverse function <em>L</em> which is
also strictly increasing and differentiable &hellip;&rdquo; is not quite correct:
consider the strictly increasing and differentiable function taking
<em>x</em> to <nobr><em>x</em><sup>3</sup></nobr>.  What's the
correct statement?  (Hint: the Chain Rule tells you what the
derivative of the inverse function must be.)

<p>

An explicit upper bound of 4 on &pi; can be obtained by calculating
<nobr>cos(2) &lt; 1 &minus; 2^2/2! + 2^4/4! = &minus;1/3 &lt; 0.</nobr>
For much better estimates, integrate
<nobr>(<em>x&minus;x</em><sup>2</sup>)<sup>4</sup></nobr>
<nobr><em>dx</em>/(<em>x</em><sup>2</sup>+1)</nobr> from 0 to&nbsp;1
and note that
<nobr>&frac12; &le; 1/(<em>x</em><sup>2</sup>+1) &le; 1.  :-)</nobr>&nbsp;&nbsp;
[That was noted in my recent Math Table talk, but attendance was depressed by
(I'm told) midterms season.]

<p>

Rudin uses a fact about convex functions that is only presented
as an exercise earlier in the book (p.100, #23).  Namely:
let <em>f</em>&thinsp; be a convex function on some interval <em>I</em>,
and consider the slope
<nobr><em>s</em>(<em>x</em>, <em>y</em>) :=
(<em>f</em>&thinsp;(<em>x</em>)&minus;<em>f</em>&thinsp;(<em>y</em>))
/ (<em>x&minus;y</em>)</nobr>
as a function on the set of <nobr>(<em>x</em>, <em>y</em>)</nobr> in
<nobr><em>I</em> &times; <em>I</em></nobr> with
<nobr><em>x</em> &gt; <em>y</em></nobr>;
then <em>s</em> is an increasing function of both variables.
The proof is fortunately not hard.  For instance, to prove that if
<nobr><em>x</em> &gt; <em>y'</em> &gt; <em>y</em></nobr> then
<nobr><em>s</em>(<em>x</em>, <em>y'</em>&thinsp;) &gt;
<em>s</em>(<em>x</em>, <em>y</em>),</nobr>
write <em>y'</em> as <nobr><em>px</em> + <em>qy</em></nobr>
with <nobr><em>p</em> + <em>q</em> = 1</nobr>,
and calculate that
<nobr><em>s</em>(<em>x</em>, <em>y'</em>&thinsp;) &gt;
<em>s</em>(<em>x</em>, <em>y</em>)</nobr>
is equivalent to the usual convexity condition.  The case
<nobr><em>x</em> &gt; <em>x'</em> &gt; <em>y</em></nobr>
works in exactly the same way.

<p>

While we alas suppress most of Fourier analysis,
we can easily prove the following: if
<nobr><em>f</em>&thinsp;:<strong>R</strong>/2&pi;<strong>Z</strong>
&rarr; <strong>C</strong></nobr>
is a continuous function whose Fourier coefficients
<nobr><em>a<sub>n</sub></em> :=
(2&pi;)<sup>&minus;1</sup>&int;<sub>
 <strong>R</strong>/2&pi;<strong>Z</strong></sub>
exp(<em>&minus;inx</em>) <em>f</em>&thinsp;(<em>x</em>) <em>dx</em></nobr>
satisfy
<nobr>&sum;<sub><em>n</em></sub>|<em>a<sub>n</sub></em>| &lt; &infin; </nobr>
then <em>f</em> equals its Fourier series.
<em>Proof</em>&thinsp;: the difference is a continuous function
all of whose Fourier coefficients vanish.  By Stone-Weierstrass, 
it can be uniformly approximated by trigonometric polynomials, etc.
(This special case of Stone-Weierstrass is also a theorem of
Fej&eacute;r, who obtained an explicit sequence of trigonometric
polynomials converging to the function; see K&ouml;rner.)
A nice example is
<nobr><em>f</em>&thinsp;(<em>x</em>)
= B<sub><em>k</em></sub>(2&pi;<em>x</em>)</nobr>
for each <em>k&ge;2</em>, where
<nobr>B<sub><em>k</em></sub></nobr> is the <em>k</em>th Bernoulli polynomial;
this yields the values of <nobr>&zeta;(<em>k</em>)</nobr> for
<nobr><em>k</em> = 2, 4, 6, &hellip;</nobr>, and much else.

<p>

The rather obscure integration by parts in Rudin, p.194 is not necessary.
A straightfoward choice of &ldquo;parts&rdquo; yields
<p>
<center>
<em>x</em> B(<em>x</em>, <em>y</em>+1) =
<em>y</em> B(<em>x</em>+1, <em>y</em>) ;
</center>
<p>
This may seem to go in a useless direction,
but the elementary observation that
<p>
<center>
B(<em>x</em>, <em>y</em>) =
B(<em>x</em>, <em>y</em>+1) + B(<em>x</em>+1, <em>y</em>)
</center>
<p>
recovers the recursion (97).

<p>

In addition to the trigonometric definite integrals noted by Rudin
(formula 98), Beta functions also turn up in the evaluation of
the definite integral of
<nobr><em>u<sup>a</sup> du</em> /
(1+<em>u<sup>b</sup></em>)<sup><em>c</em></sup></nobr>
over <nobr>(0,&infin;)</nobr>: let
<nobr><em>t</em> = <em>u<sup>b</sup></em> / (1+<em>u<sup>b</sup></em>)</nobr>.
What is the value of that integral?
Can you obtain in particular the formula
<nobr>&pi; / (<em>b</em> sin(<em>a</em> &pi;/<em>b</em>))</nobr>
for the special case <em>c</em>=1?

<p>

The limit formula for &Gamma;(<em>x</em>) readily yields
the product formula:
<blockquote>
&Gamma;(<em>x</em>) = 
<em>x</em><sup>&minus;1</sup> <em>e</em><sup>&minus;<em>Cx</em></sup>
Prod(exp(<em>x</em>/<em>k</em>) / (1+<em>x</em>/<em>k</em>),
<em>k</em>=1,2,3,...)
</blockquote>
where <em>C</em>=0.57721566490... is <em>Euler's constant</em>,
the limit as <nobr><em>N</em>&rarr;&infin;</nobr> of
<nobr>1 + (1/2) + (1/3) + ... + (1/<em>N</em>) &minus; log(<em>N</em>)</nobr>.
This lets us easily show that &Gamma; is infinitely differentiable
(in fact analytic) and to obtain nice formulas for the derivatives
of log(&Gamma;(<em>x</em>)); for instance,
<nobr>&Gamma;'(1)=&minus;<em>C</em></nobr>, and more generally
the logarithmic derivative of <nobr>&Gamma;(<em>x</em>)</nobr> at
<nobr><em>x</em> = <em>N</em>+1</nobr> is
<nobr>1 + (1/2) + (1/3) + ... + (1/<em>N</em>) &minus; <em>C</em>.</nobr>

<p>

<img src="redball.gif">
We next begin <strong>multivariate differential calculus</strong>,
starting at the middle of Rudin Chapter 9 (since the first part
of that chapter is for us a review of linear algebra &mdash;
but you might want to read through the material on norms of linear maps
and related topics in pages 208&ndash;9).
Again, Rudin works with functions from open subsets of
<strong>R</strong><sup><em>n</em></sup>
to <strong>R</strong><sup><em>m</em></sup>,
but most of the discussion works equally well with the target space
<nobr><strong>R</strong><sup><em>m</em></sup></nobr>
 replaced by an arbitrary normed vector space&nbsp;<em>V</em>.
  If we want to allow arbitrary
normed vector spaces for the <em>domain</em> of&nbsp;<em>f</em>,
 we'll usually have to require that the derivative <em>f</em>&thinsp;'
 be a <em>continuous</em> linear map, or equivalently that its norm
<nobr>||<em>f</em>&thinsp;'|| =
 sup<sub>|<em>v</em>|&le;1</sub>|<em>f</em>&thinsp;'(<em>v</em>)|</nobr>
be finite.

<p>

As in the <A HREF="#HahnBanach">univariate case</A>,
proving the Mean Value Theorem in the multivariate context
(Theorem 9.19) requires either that <em>V</em> have an inner-product
norm, or the use of the Hahn-Banach theorem
to construct suitable functionals on <em>V</em>.  Once this is done,
the key Theorem 9.21 can also be proved for functions to <em>V</em>,
and without first doing the case <em>m</em>=1.  To do this,
first prove the result in the special case when each
<nobr><em>D<sub>j</sub> f</em>&thinsp;(<strong>x</strong>)</nobr>
vanishes; then reduce to this case by subtracting from <em>f</em>
the linear map from <nobr><strong>R</strong><sup><em>n</em></sup></nobr>
to&nbsp;<em>V</em> indicated by the partial derivatives
<nobr><em>D<sub>j</sub> f</em>&thinsp;(<strong>x</strong>).</nobr>

<p>

The <em>Inverse function theorem</em> (9.24) is a special case
of the <em>Implicit function theorem</em> (9.28), and its
proof amounts to specializing the proof of the implicit function
theorem.  But Rudin proves the Implicit theorem as a special
case of the Inverse theorem, so we have to do Inverse first.
(NB for these two theorems we will assume
that our target space is finite-dimensional;
how far can you generalize to infinite-dimensional spaces?)
Note that Rudin's statement of the contraction principle
(Theorem 9.23 on p.220; cf.&nbsp;the final problem of
<A HREF="p5.pdf" target="_blank">problem set 5</A>)
is missing the crucial hypothesis that <em>X</em> be nonempty!
The end of the proof of 9.24 could be simplified if Rudin allowed
himself the full use of the hypothesis that <strong>f</strong>
is continuously differentiable on&nbsp;<em>E</em>,
not just at&nbsp;<strong>a</strong>: differentiability of the
inverse function&nbsp;<strong>g</strong> at
<nobr><strong>b</strong> = <strong>f</strong>(<strong>a</strong>)</nobr>
is easy given Rudin's construction of&nbsp;<strong>g</strong>;
differentiability at any other point
<nobr><strong>f</strong>(<strong>x</strong>)</nobr> follows,
since <strong>x</strong> might as well be <strong>a</strong>,
and then the derivative is continuous because
<strong>g</strong> and <nobr><strong>f</strong>&thinsp;' are.</nobr>

<p>

The proof of the second part of the implicit function theorem,
which asserts that the implicit function <strong>g</strong> not only
exists but is also continuously differentiable with derivative
at <strong>b</strong> given by formula (58) (p.225), can be done
more easily using the chain rule, since <strong>g</strong> has been
constructed as the composition of the following three functions:
first, send <strong>y</strong> to
<nobr>(<strong>0</strong>, <strong>y</strong>); then,
apply the inverse function
<nobr><strong>F</strong><sup>&minus;1</sup>;</nobr>
finally, project the resulting vector
<nobr>(<strong>x</strong>,<strong>y</strong>)</nobr>
to&nbsp;<strong>x</strong>.  The first and last of these three functions
are linear, so certainly <em>C</em><sup>1</sup>; and the continuous
differentiability of <strong>F</strong><sup>&minus;1</sup> comes from
the inverse function theorem.

<p>

Here's an approach to <em>D<sub>ij</sub>=D<sub>ji</sub></em>
that works for a <em>C</em><sup>2</sup> function to an arbitrary
normed space.  As in Rudin (see p.235) we reduce to the case of
a function of two variables, and define <em>u</em> and &Delta;.
Assume first that <em>D</em><sub>21</sub> <em>f</em> vanishes
at (<em>a,b</em>).  Then use the Fundamental Theorem of Calculus
to write &Delta;(<em>f,Q</em>) as as the integral of
<nobr><em>u'</em>(<em>t</em>) <em>dt</em></nobr> on
<nobr>[<em>a</em>, <em>a+h</em>]</nobr>,
and then write <nobr><em>u'</em>(<em>t</em>)</nobr> as an integral of 
<nobr><em>D</em><sub>21</sub>
<em>f</em>&thinsp;(<em>t,s</em>) <em>ds</em></nobr>
on <nobr>[<em>b</em>,<em>b</em>+<em>k</em>]</nobr>.  Conclude that
</nobr><em>u'</em>(<em>t</em>) = <em>o</em>(<em>k</em>)</nobr>
and thus that
<nobr>&Delta;(<em>f,Q</em>) / <em>hk</em></nobr> approaches zero.
Now apply this to the function
<nobr><em>f</em> &minus; <em>xyD</em><sub>21</sub>
 <em>f</em>&thinsp;(<em>x</em>,<em>y</em>)</nobr>
to see that in general &Delta;(<em>f,Q</em>) / <em>hk</em> approaches
<em>D</em><sub>21 </sub><em>f</em>&thinsp;(<em>x</em>,<em>y</em>).
 Do the same in reverse order to conclude that
<nobr><em>D</em><sub>21</sub>
<em>f</em>&thinsp;(<em>x</em>,<em>y</em>)=<em>D</em><sub>12</sub>
<em>f</em>&thinsp;(<em>x</em>,<em>y</em>).
</nobr>
Can you prove
<nobr><em>D</em><sub>12</sub>(<em>f</em>&thinsp;)
= <em>D</em><sub>21</sub>(<em>f</em>&thinsp;)</nobr>
for a function <em>f</em> to an arbitrary inner product space
under the hypotheses of Theorem 9.41?

<p>

We omit the &ldquo;rank theorem&rdquo; (whose lesser importance
is noted by Rudin himself), as well as the section on determinants
(which we treated at much greater length in Math&nbsp;55a).

<p>

An important application of iterated partial derivatives is the
Taylor expansion of an <nobr><em>m</em>-times</nobr> differentiable
function of several variables; see Exercise 30 (Rudin, 243-244).
As promised at the start of Math 55a and/or Math 55b, this also
applies to maxima and minima of real-valued functions <em>f</em>&thinsp;
of several variables, as follows.  If <em>f</em>&thinsp;
is differentiable at a local maximum or minimum
then its derivative there vanishes, as was the case
for a function of one variable.  Again we say that
a zero of the derivative is a &ldquo;critical point&rdquo;
of&nbsp;<em>f</em>.  Suppose now that <em>f</em>&thinsp; is
<nobr><em>C</em><sup>2</sup></nobr> near a critical point.
The second derivative can be regarded as a quadratic form.
It must be positive semidefinite at a local minimum,
and negative semidefinite at a local maximum.  Conversely,
if it is strictly positive (negative) definite at a critical point
then that point is a strict local minimum (maximum) of&nbsp;<em>f</em>.
Compare with Rudin's exercise 31 on page 244 (which however assumes that
&thinsp;<em>f</em>&thinsp; is <nobr><em>C</em><sup>3</sup></nobr> &mdash;
I don't know why Rudin requires third partial derivatives).

<p>

<img src="redball.gif">

Next topic, and last one from Rudin, is
<strong>multivariate integral calculus</strong> (Chapter 10).
Most of the chapter is concerned with setting up a higher-dimensional
generalization of the Fundamental Theorem of Calculus that comprises
the divergence, Stokes, and Green theorems and much else besides.
With varying degrees of regret we'll omit this material, as well as
the Lebesgue theory of Chapter&nbsp;11.  We will, however, get
some sense of multivariate calculus by giving a definition of
integrals over <nobr><strong>R</strong><sup>n</sup></nobr>
and proving the formula for change of variables (Theorem 10.9).
this will already hint why in general an integral over an
<nobr><em>n</em>-dimensional</nobr> space is often best viewed
as an integral not of a function but a &ldquo;differential
<nobr><em>n</em>-form</nobr>&rdquo;.  For instance, in two dimensions
an integral of
<nobr>&ldquo;<em>f</em>&thinsp;(<em>x</em>,&thinsp;<em>y</em>)
 <em>dx dy</em>&rdquo;</nobr>
can be thought of as
<nobr>&ldquo;<em>f</em>&thinsp;(<em>x</em>,&thinsp;<em>y</em>)
 <em>dx</em> &and; <em>dy</em>&rdquo;</nobr>,
and then we recover the formula involving the Jacobian from
the rules of exterior algebra.  You'll have to read the
rest of this chapter of Rudin, and/or take a course on
differential geometry or &ldquo;calculus on manifolds&rdquo;,
to see these ideas developed more fully.

<p>

Once we have the change of variables formula, we'll return to the
section of Chapter&nbsp;8 concerning the Euler's Beta and Gamma
integrals and give a more natural treatment of the formula
relating them (Theorem&nbsp;8.20).

<p>

<img src="redball.gif">
Math 55b concludes with an introduction to
<strong>complex analysis</strong>
(a.k.a. &ldquo;functions of one complex variable&rdquo;).
We'll start with contour integrals and the fundamental
theorems of Cauchy, roughly following the exposition in Ahlfors,
chapter&nbsp;III (p.82&nbsp;ff.).  We'll prove:
<ul>
<li> Cauchy's theorem for a rectangle (if <em>f</em>&thinsp; is
 differentiable on a neighborhood of the rectangle, then the contour
 integral of <nobr><em>f</em>&thinsp;(<em>z</em>)&thinsp;<em>dz</em></nobr>
 on the boundary of the rectangle vanishes),
 using the subdivision trick that Ahlfors attributes to Goursat.
<li> The variant when <em>f</em>&thinsp; is defined on the complement
 in the rectangle of finitely many points &zeta;, as long as
 <nobr>(<em>z</em>&minus;&zeta;) <em>f</em>&thinsp;(<em>z</em>) &rarr; 0</nobr>
 on each such point
<li> Same theorems for a circle, using the same subdivision trick.
<li> Likewise for the annulus between two concentric circles: the
 integrals over the two circles are equal.  (One proof: change
 variables to a rectangle of height 2&pi; using complex exponential.)
<li> Cauchy's integral formula for a rectangle or circle:
 under the same hypotheses,
 if <em>a</em> is any interior point then
 <nobr><em>f</em>&thinsp;(<em>a</em>)<nobr>
 is <nobr>(1/2&pi;<em>i</em>)</nobr> times the contour integral of
 <nobr><em>f</em>&thinsp;(<em>z</em>)&thinsp;<em>dz</em>
  / (<em>z</em>&minus;<em>a</em>)</nobr>.
<li> Consequences:
 <ol>
  <li> If <em>f</em>&thinsp; is differentiable in a circle of radius
   <nobr><em>R</em> &gt; <em>r</em> &gt; 0</nobr> about&nbsp;<em>a</em>
   then <em>f</em>&thinsp;(<em>a</em>) is the average of
   <nobr><em>f</em>&thinsp;(<em>a</em>
    + <em>re</em><sup><em>i</em>&theta;</sup>)
   </nobr>
   over &theta; in <nobr>[0,&thinsp;2&pi;]</nobr>; corollary(!):
   Fundamental Theorem of Algebra; also:
   <nobr>|<em>f</em>&thinsp;(&middot;)|</nobr> has no local maximum
   unless it is constant
   (&ldquo;maximum principle&rdquo; for analytic functions).
  <li> [via power series expansion of
   <nobr>1&thinsp;/&thinsp;(<em>z</em>&minus;<em>a</em>)</nobr>]:
   <em>f</em>&thinsp; is analytic, and its power-series expansion about
   any <nobr><em>z</em><sub>0</sub></nobr> converges in
   <nobr>|<em>z</em>&minus;<em>z</em><sub>0</sub>| &lt; <em>r</em></nobr>
   if that open circle is contained in the interior of our rectangular
   or circular contour.  This again yields the maximum principle above,
   and also shows that
   <nobr>|<em>f</em>&thinsp;(&middot;)|</nobr> has no <em>nonzero</em>
   local <u>minimum</u> unless it is constant.
  <li> Same argument (or contour integration by parts) also yields
   an integral formula for the derivative
   <nobr><em>f</em>&thinsp;'(<em>z</em><sub>0</sub>)</nobr>, which
   in turn gives Liouville's theorem: a bounded analytic function on
   all of&nbsp;<strong>C</strong> is constant.  [An analytic function
   on all of&nbsp;<strong>C</strong> is also known as an
   <em>entire function</em>.]
  <li> Using also the integral formula for higher derivatives of
   an analytic function: The uniform limit <em>f</em>&thinsp;
   of a sequence <nobr>{<em>f<sub>n</sub></em>}</nobr>
   of analytic functions is analytic,
   and each term in a power series expanion of&nbsp;<em>f</em>&thinsp;
   is the limit of the corresponding terms for the
   <nobr><em>f<sub>n</sub></em></nobr>.  Thus the same is true
   for a uniformly convergent sum of analytic functions.
   This is very useful for constructing/defining analytic functions;
   e.g. the Riemann zeta function <nobr>&zeta;(<em>s</em>)</nobr>
   is defined for <nobr>Re(<em>s</em>)&gt;1</nobr> as
   <nobr>&sum;<sub><em>n</em>&thinsp;</sub><em>n<sup>&minus;s</sup></em></nobr>
   (summed over <nobr><em>n</em>=1,2,3,&hellip;</nobr>).
 </ol>
<li> (&ldquo;analytic continuation&rdquo;)
 An analytic function&nbsp;<em>f</em>
 on a neighborhood of&nbsp;<em>z</em> that vanishes on distinct points
 <nobr><em>z<sub>n</sub></em> &rarr; <em>z</em></nobr>
 is identically zero.  (Proof: by induction each coefficient in the
 power series expansion of&nbsp;<em>f</em> about&nbsp;<em>z</em>
 vanishes.)  Hence if two analytic functions
 <nobr><em>f</em>, <em>g</em></nobr> agree at each
 <nobr><em>z<sub>n</sub></em></nobr> then they are equal.
<li> Cauchy's integral formula also works under the weaker hypothesis
 we've seen above, allowing a finite number of exceptional &zeta;;
 the formula shows that in this case <em>f</em>&thinsp; extends to
 an analytic function on all of the interior of our rectangular or
 circular contour.
 Thus such &zeta; is called a <em>removable singularity</em>.
<li> More generally, if <em>f</em>&thinsp; is analytic in a
 punctured neighborhood of&nbsp;&zeta;, and
 <nobr>(<em>z</em>&minus;&zeta;)<sup><em>n</em></sup>
  <em>f</em>&thinsp;(<em>z</em>) &rarr; 0</nobr>
 for some positive integer&nbsp;<em>n</em>, then
 <nobr>(<em>z</em>&minus;&zeta;)<sup><em>n</em>&minus;1</sup>&thinsp;<em>f</em>&thinsp;(&middot;)</nobr>
 has a removable singularity at&nbsp;&zeta;.  Then
 <em>f</em>&thinsp; is said to have a <em>pole</em> at&nbsp;&zeta;:
 we can write <nobr><em>f</em>&thinsp;(<em>z</em>)</nobr>
 as an polynomial of degree &lt;<em>n</em> in
 <nobr>1&thinsp;/&thinsp;(<em>z</em>&minus;&zeta;)</nobr>
 plus an analytic function on a neighborhood of&nbsp;&zeta;.
 We say &zeta; is a single, double, triple, etc.&nbsp;pole
 if that polynomial in
 <nobr>1&thinsp;/&thinsp;(<em>z</em>&minus;&zeta;)</nobr>
 has degree 1, 2, 3, &hellip;, or a pole of order&nbsp;<em>d</em>
 if the polynomial has degree&nbsp;<em>d</em>.
 (A removable singularity then has <em>d</em>=0
 but is rarely called a &ldquo;pole of order zero&rdquo;.)
 If there's no such&nbsp;<em>n</em> then <em>f</em>&thinsp;
 is said to have an <em>essential singularity</em> at&nbsp;&zeta;.
 The standard example of such a function is
 <nobr><em>f</em>&thinsp;(<em>z</em>) = exp(1/<em>z</em>)</nobr>
 with <nobr>&zeta; = 0</nobr>.
<li> If <em>f</em>&thinsp; is analytic on an open set <em>E</em>,
 and has a set <em>P</em> of poles, then <em>f</em> is said to be
 <em>meromorphic</em> on the union of <em>E</em> and&nbsp;<em>P</em>.
 In the special case of an analytic function (i.e.&nbsp;with <em>P</em>
 empty) we also say <em>f</em>&thinsp; is <em>holomorphic</em>.
 As the holomorphic functions form a ring, the meromorphic functions
 on a subset of&nbsp;<strong>C</strong> form a field; it turns out
 to be the fraction field of the holomorphic functions: for any
 meromorphic function&nbsp;<em>f</em>&thinsp; we can find
 a holomorphic function <em>h</em> that vanishes at the poles
 of <em>f</em>&thinsp; to the necessary multiplicities for
 <nobr><em>g</em>=<em>hf</em>&thinsp;</nobr> to be holomorphic,
 and then <nobr><em>f</em>&thinsp;=</em>g</em>/<em>h</em></nobr>.
 [NB: Analytic continuation also works for meromorphic functions;
 indeed it's easy to see that a function continuous at&nbsp;<em>z</em>
 cannot have a sequence of poles approaching&nbsp;<em>z</em>,
 so by restricting to a small enough neighborhood we get
 a holomorphic function.]
<li> Example: if <em>f</em>&thinsp; is analytic on the unit disc
 and vanishes at the origin then <nobr><em>f</em>/<em>z</em></nobr>
 is analytic too; this yields the
 <a href="http://en.wikipedia.org/wiki/Schwarz_lemma"
 target="_blank">Schwarz Lemma</a>.
</ul>
<img src="orangeball.gif">
<A NAME="today">
A key concept in the theory and application of complex analysis
is the residue of a function &mdash; more properly, a differential
&mdash; on a punctured neighborhood of some complex number.
<ul>
<li>
 For an analytic function <em>f</em>&thinsp; on a punctured
 neighborhood of &zeta;, the <em>residue</em> at&nbsp;&zeta; of the
 <em>differential</em>
 <nobr><em>f</em>&thinsp;(<em>z</em>) <em>dz</em></nobr>
 is invariant under locally invertible analytic changes of variable;
 i.e. is the same as the residue of
 <nobr><em>f</em>&thinsp;(<em>g</em>(<em>w</em>))
  <em>g</em>'(<em>w</em>) <em>dw</em></nobr>
 at the preimage of&nbsp;&zeta; under&nbsp;<em>g</em>,
 assuming <em>g</em>' does not vanish there.
 <nobr>The residue is defined as the integral of
 (1/2&pi;<em>i</em>) <em>f</em>&thinsp;(<em>z</em>) <em>dz</em></nobr> 
 on (say) a circle around&nbsp;&zeta;.  If
 <nobr><em>f</em>&thinsp;(<em>z</em>)</nobr>
 has a power series expansion
 <nobr>&sum;<sub><em>n</em>&thinsp;</sub><em>c<sub>n</sub></em>
  (<em>z</em>&minus;&zeta;)<sup><em>n</em></sup>
 </nobr> in a punctured neighborhod of&nbsp;&zeta;
 then the residue is the coefficient
 <nobr><em>c</em><sub>&minus;1</sub></nobr>.
<li>
 The integral of a differential on a closed contour (at least one of our
 standard contours: circles, rectangles, and their images under
 invertible conformal maps) is 2&pi;<em>i</em> times the sum of
 the residues at the poles or essential singularities the contour
 encloses, assuming the differential is analytic except at those
 finitely many points.
<li> This has numerous applications to the evaluation of (often
 non-elementary) definite integrals of elementary functions.
 Some paradigmatic examples:
 <ul>
 <li>
  &int;<sub>&thinsp;0</sub><sup>&infin;</sup>
   <em>dx</em> / (<em>x</em><sup>2</sup> + 1) &nbsp;=&nbsp; &pi; / 2
 <li>
  &int;<sub>&thinsp;0</sub><sup>&infin;</sup>
   <em>dx</em> / (<em>x</em><sup>2</sup> + 1)<sup>2</sup>
    &nbsp;=&nbsp; &pi; / 4
 <li>
  &int;<sub>&thinsp;0</sub><sup>&infin;</sup>
   cos(<em>cx</em>)
   <em>dx</em> / (<em>x</em><sup>2</sup> + 1)
   &nbsp;=&nbsp; (&pi;&thinsp;/&thinsp;2) <em>e</em><sup>&minus;|<em>c</em>|</sup>
   &nbsp; for all real&nbsp;<em>c</em>
 <li>
  &int;<sub>&thinsp;0</sub><sup>&infin;</sup>
   sin(<em>cx</em>)
   <em>dx</em> / <em>x</em>
   &nbsp;=&nbsp; &pi; / 2
   &nbsp; for all <nobr><em>c</em>&thinsp;&gt;&thinsp;0</nobr>
   [via the &ldquo;principal value&rdquo; of the integral of
   <nobr>exp(<em>icx</em>) <em>dx</em>&thinsp;/&thinsp;<em>x</em></nobr>
   over <nobr>(&minus;&infin;,&infin;)</nobr>]
 <li>
  For &alpha; in (0,1), the special value
  B(&alpha;, 1&minus;&alpha;) of the Beta function is given by
  <br>
   &int;<sub>&thinsp;0</sub><sup>1</sup>
   <em>dx</em> / (<em>x</em><sup>&alpha;</sup>(<em>x</em> + 1))
    &nbsp;=&nbsp; &pi; / sin(&alpha;&pi;)
 </ul>
<li> An important special case: the <em>logarithmic differential</em>
 <nobr><em>df</em> / <em>f</em></nobr> of a meromorphic
 function&nbsp;<em>f</em>&thinsp; has only simple poles, at the
 zeros and poles of&nbsp;<em>f</em>&thinsp;, with residue <em>n</em>
 at a zero of order&nbsp;<em>n</em> and <nobr>&minus;<em>n</em></nobr>
 at a pole of order&nbsp;<em>n</em>.  Hence the integral of
 <nobr><em>df</em> / <em>f</em> =
  (<em>f</em>&thinsp;'(<em>z</em>) / <em>f</em>&thinsp;(<em>z</em>))
  <em>dz</em> </nobr>
 on a closed contour is 2&pi;<em>i</em> times the difference between
 the number of zeros and poles enclosed by the contour, counted with
 multiplicity.  This assumes that <em>f</em>&thinsp; is analytic on
 the interior of the contour and has neither zero nor pole on
 the contour itself.  This formula is often called the
 &ldquo;argument principle&rdquo;, because the integral can be
 interpreted as <em>i</em> times the change of the &ldquo;argument&rdquo;
 (=&nbsp;imaginary part of the logarithm) of&nbsp;<em>f</em>&thinsp;
 around the contour.  A nice and occasionally useful corollary is
 <em>Rouch&eacute;'s theorem</em>: if
 <nobr>|<em>g</em>(<em>z</em>)| &lt; |<em>f</em>&thinsp;(<em>z</em>)|</nobr>
 for every <em>z</em> <u>on</u> the contour, then
 <nobr><em>f</em>&thinsp;+</em>g</em></nobr> has the same change of
 argument around the contour as&nbsp;<em>f</em>, and thus the same
 count of zeros minus poles with multiplicity on the <u>interior</u>
 of the contour.
 <li>The <em>Gamma function</em> extends to a meromorphic function
 on <strong>C</strong>, satisfying the same functional equation
 <nobr>&Gamma;(<em>z</em>+1) = <em>z</em> &Gamma;(<em>z</em>)</nobr>,
 and holomorphic except for simple poles at
 <nobr><em>z</em> = 0, &minus;1, &minus;2, &minus;3, &hellip; .</nobr>
 Given that there exists a meromorphic function
 &Gamma; on <strong>C</strong> that agrees with the usual
 Gamma function on the positive real axis, this extension is unique
 (analytic continuation again), but it is not immediate that
 such an extension is possible. We give several approaches:
 <ol>
 <li>Use the Euler integral to define <nobr>&Gamma;(<em>z</em>)</nobr>
  for <nobr><em>z</em> = <em>x</em> + <em>iy</em></nobr>
  of positive real part&nbsp;<em>x</em>; prove the functional equation
  for such <em>x</em> (either by analytic continuation or by the
  usual integration by parts); and then use the functional equation
  to inductively extend &Gamma; to
  <nobr><em>x</em> &gt; &minus;1</nobr>,
  <nobr><em>x</em> &gt; &minus;2</nobr>,
  <nobr><em>x</em> &gt; &minus;3</nobr>, etc.,
  at each stage finding a simple pole at
  <nobr>0, &minus;1, &minus;2, etc.</nobr>
 <li>Use the formula
  <nobr>&Gamma;(<em>z</em>) =
   lim<sub><em>n</em>&rarr;&infin;</sub>
    <em>n</em>! <em>n<sup>z</sup></em> <font size="+1">/</font>
     (<em>z</em> (<em>z</em>+1) (<em>z</em>+2) &hellip;
      (<em>z</em>+<em>n</em>)) </nobr>.
   This sequence of analytic functions converges uniformly
   on bounded subsets of
   <nobr><strong>C</strong> &minus;
    {0, &minus;1, &minus;2, &minus;3, &hellip;}</nobr>,
   whence its limit is analytic.
  <li> Let <em>C</em> be a contour that goes from
   <nobr>&ldquo;&infin; &minus; <em>i</em>&epsilon;&rdquo;</nobr>
   horizontally
   <nobr>to &minus;&epsilon;&minus;<em>i</em>&epsilon;,</nobr>
   then vertically to
   <nobr>to &minus;&epsilon;+<em>i</em>&epsilon;,</nobr>
   and horizontally to
   <nobr>to &ldquo;&infin; + <em>i</em>&epsilon;&rdquo;.</nobr>
   The integral along&nbsp;<em>C</em> of
   <nobr><em>t</em><sup>&thinsp;<em>z</em>&minus;1</nobr>
    <em>e</em><sup>&minus;<em>t</em></sup> <em>dt</em></nobr>
   defines an entire function of&nbsp;<em>z</em> that equals
   <nobr>(1&minus;<em>e</em><sup>2&pi;<em>iz</em></sup>) &Gamma;(<em>z</em>)</nobr>
   for <nobr>Re(<em>z</em>) &gt; 0</nobr>, so we can solve for
   <nobr>&Gamma;(<em>z</em>)</nobr> for any complex&nbsp;<em>z</em>.
 </ol>
 Each of these can be used to prove the functional equation; the second
 most easily yields the following additional properties:
 <ul>
 <li>For each <em>x</em>&thinsp;&gt;&thinsp;0, the function
  <nobr>|&thinsp;&Gamma;(<em>x</em>+<em>iy</em>)&thinsp;|</nobr>
  of a real variable&nbsp;<em>y</em> is maximized at
  <nobr><em>y</em>&thinsp;=&thinsp;0</nobr>,
  and is increasing for
  <nobr><em>y</em>&thinsp;&ge;&thinsp;0</nobr>
  and decreasing for
  <nobr><em>y</em>&thinsp;&le;&thinsp;0</nobr>;
 <li>In particular, &Gamma;(<em>z</em>) &ne; 0
  for all complex <em>z</em>;
 <li>The Stirling approximation holds for the complex Gamma function,
  in the following sense:
  given <nobr>&epsilon;&thinsp;&gt;&thinsp;0</nobr>,
  we have &Gamma;(<em>z</em>) asymptotic to
  <nobr>(2&pi;)<sup>1/2</sup> <em>z</em><sup><em>z</em>&minus;(1/2)</sup>
   <em>e</em><sup>&minus;<em>z</em></nobr> as
   <nobr>|<em>z</em>|&thinsp;&rarr;&infin;&thinsp;</nobr>
   as long as <em>z</em> has argument in
 <!-- <nobr>[&pi;&minus;&epsilon;, &epsilon;&minus;&pi;]</nobr>. -->
 <!-- oops...  spotted by L.Alpoge --NDE] -->
   <nobr>[&epsilon;&minus;&pi;, &pi;&minus;&epsilon;]</nobr>.
 </ul>
 [NB We did not prove this last part in class, and yes, it does
 reduce to the usual Stirling approximation to <em>n</em>! for
 <nobr><em>z</em> = <em>n</em>+1</nobr>
 even though it looks different.]
 <li>The Beta identity
 <nobr>B(<em>x</em>,<em>y</em>)
  =
  &Gamma;(<em>x</em>) &Gamma;(<em>y</em>)
  / &Gamma;(<em>x</em>+<em>y</em>)</nobr>
 still holds for <em>x</em> and <em>y</em> of positive real part,
 and can be used to define <nobr>B(<em>x</em>,<em>y</em>)</nobr>
 for all complex <em>x</em> and <em>y</em> not among the
 poles of&nbsp;&Gamma;.  In particular, the identity
 <nobr> B(&alpha;, 1&minus;&alpha;) =
   &nbsp;=&nbsp; &pi;&thinsp;/&thinsp;sin(&alpha;&pi;)</nobr>
 holds for all non-integer complex numbers&nbsp;&alpha;.
 We used this, together with the formula
  <nobr>&Gamma;(<em>z</em>) =
   lim<sub><em>n</em>&rarr;&infin;</sub>
    <em>n</em>! <em>n<sup>z</sup></em> <font size="+1">/</font>
     (<em>z</em> (<em>z</em>+1) (<em>z</em>+2) &hellip;
      (<em>z</em>+<em>n</em>)) </nobr>,
 to give a proof (admittedly more convoluted than necessary or usual)
 of the product expansion of the sine, and thus (via logarithmic
 differentiation) of the partial-fraction decomposition of
 <nobr>cot(<em>x</em>)</nobr> and (differentiating once more)
 <nobr>sec<sup>2</sup>(<em>x</em>)</nobr>, etc.  Evaluation at
 special points like &pi;/2 yields classical formulas: Wallis's
 product, the values of <nobr>&zeta;(<em>k</em>)</nobr> for
<nobr><em>k</em> = 2, 4, 6, &hellip;</nobr> (again), and others.
 <li> (not covered in the final exam) Introduction to conformal mapping
 and the <a href="http://en.wikipedia.org/wiki/Riemann_mapping_theorem"
 target="_blank">Riemann mapping theorem</a>; and 1.5 proofs of
 <a href=http://en.wikipedia.org/wiki/Muntz-Szasz_theorem
 target="_blank">M&uuml;ntz's generalization</a> of the
 Weierstrass approximation theorem.
</ul>

<hr>

<img src="redball.gif">
<A HREF="p1.pdf">Problem sets 1 and 2</A>: Metric topology basics
<br>
<strong>Problem 1 postponed</strong> to Problem Set 2 because
we probably won't define &ldquo;dense&rdquo; Wednesday.
You can already look at Problems 6 and 7, which don't require
anything beyond Math&nbsp;55a plus the notion of a metric,
but you need not hand either of them in with PS1.
<br>
<img src="purpball.gif">
About <strong>Problem 1</strong> and &ldquo;dense&rdquo;:
a subset <em>E</em> of a topological space <em>X</em>
is said to be <em>dense</em> if its closure is <em>X</em>,
i.e. if <em>X</em> is the only closed set containing <em>E</em>.
For a subspace <em>Y</em> of&nbsp;<em>X</em> we say
<em>E</em> is dense in <em>Y</em>
if its closure contains&nbsp;<em>Y</em>.
<br>
<font color="red">NB</font>
About <strong>Problem 5</strong>: here, and in general,
<strong>R</strong> is assumed to have its usual metric
<nobr><em>d</em>(<em>x</em>,<em>y</em>)=|<em>x</em>&minus;<em>y</em>|</nobr>,
unless (as in Problem&nbsp;3) a different metric is specified.
<br>
<img src="purpball.gif">
Problem 12 <strong>corrected</strong> 27.i.11: the metric space that
must consist only of <em>closed</em> bounded nonempty subsets
of&nbsp;<em>X</em> is not <em>X</em> itself&hellip;
 <!-- L.Alpoge -->
<br>
<img src="yellowball.gif">For Problem 6:
the isometries are all of the form
<nobr><em>i</em>(<em>x</em>) = <em>Ax</em> + <em>b</em></nobr>
where <em>b</em> is any vector and <em>A</em> is a linear transformation
that's orthogonal for 6i and a signed permutation matrix for 6ii.  It's
easy to see that any such map works (and indeed translation by <em>b</em>
is an isometry of any normed vector space), so the hard part is showing
that there are no others.  Using translation, it's enough to prove this
for isometries taking 0 to&nbsp;0. The basic strategy in such a problem
is to reconstruct the vector-space structure of&nbsp;<em>V</em> from
the metric.
<br>
<img src="yellowball.gif"><img src="yellowball.gif">
For an inner-product norm (Problem 6i) it's easy because collinear points can be
recognized from having equality in the triangle inequality;
geometrically they form degenerate triangles.  For example,
for any vector <em>x</em> and scalar <em>r</em>
we can characterize <em>rx</em> by the conditions
<nobr><em>d</em>(0,&thinsp;<em>rx</em>) = |<em>r</em>| <em>d</em>(0,&thinsp;<em>x</em>)</nobr>
and
<nobr><em>d</em>(<em>x</em>,&thinsp;<em>rx</em>) = |<em>r</em>&minus;1| <em>d</em>(0,&thinsp;<em>x</em>).</nobr>
Hence any isometry <em>i</em> that takes 0 to&nbsp;0 must take <em>rx</em> to
<nobr><em>r&thinsp;i</em>(<em>x</em>).</nobr>  Likewise the midpoint
<nobr>(<em>x</em>+<em>y</em>)/2</nobr> is characterized by its distances from
<em>x</em> and&nbsp;<em>y</em>, so <em>i</em> must respect averages,
and thus also respects vector sums because we've already seen it takes 
2<em>x</em> to to <nobr>2<em>&thinsp;i</em>(<em>x</em>).</nobr>
In other words, <em>i</em> is a linear transformation.  We've already seen
that a <em>linear</em> isometry is an orthogonal transformation, which lets us
finish this solution of&nbsp;6i.
<br>
[Note however that if <em>V</em> is a <em>complex</em> inner product space
then its isometry group contains maps
<nobr><em>i</em>(<em>x</em>) = <em>Ax</em> + <em>b</em></nobr>
where <em>A</em> is linear over&nbsp;<strong>R</strong>
but not over&nbsp;<strong>C</strong>.]
<br>
<img src="yellowball.gif"><img src="yellowball.gif">
For the sup norm (Problem 6ii) it's trickier because there are many more degenerate triangles.
Geometrically, &ldquo;spheres&rdquo; of radius&nbsp;<em>r</em> centered at points at distance
2<em>r</em> can be &ldquo;tangent&rdquo; at many points.  But we can turn this to our
advantage by exploiting the structure of the sets, call them
<nobr><em>T</em>(<em>p</em>,<em>q</em>),</nobr> where the spheres of radius
<nobr><em>d</em>(<em>p</em>,<em>q</em>)/2</nobr> intersect, i.e. the sets of points
at distance <nobr><em>d</em>(<em>p</em>,<em>q</em>)/2</nobr> from both
<em>p</em> and&nbsp;<em>q</em>.  Here's one way of doing this: given <em>p</em> and <em>r</em>
there's a set, call it <nobr><em>C<sub>r</sub></em>(<em>p</em>),</nobr>
of <nobr>2<sup><em>n</em></sup></nobr> points <em>q</em> such that
<nobr><em>d</em>(<em>p</em>,<em>q</em>)=<em>r</em></nobr> and
<nobr><em>T</em>(<em>p</em>,<em>q</em>)</nobr> consists of a single point
(namely those for which each coordinate of
<nobr><em>q</em>&minus;<em>p</em></nobr> is <nobr>&plusmn<em>r</em>).</nobr>
Thus an isometry <em>i</em> that fixes <em>p</em> must permute those 
<nobr>2<sup><em>n</em></sup></nobr> points.  Now there are <nobr>2<em>n</em></nobr>
points <nobr><em>p</em>'</nobr> at distance <nobr>2<em>r</em></nobr> from&nbsp;<em>p</em>
such that <nobr><em>C<sub>r</sub></em>(<em>p</em>')</nobr>
intersects <nobr><em>C<sub>r</sub></em>(<em>p</em>)</nobr> in
<nobr>2<sup><em>n</em>&minus;1</sup></nobr> points 
(namely those for which <nobr><em>q</em>&minus;<em>p</em></nobr> is
<nobr>&plusmn;2<em>r</em></nobr> times a coordinate unit vector).
So <em>i</em> must permute those <nobr>2<em>n</em></nobr> points&nbsp;<em>p</em>'
as well for each choice of <em>p</em> and&nbsp;<em>r</em>.
It soon follows that if <nobr><em>i</em>(0)=0</nobr> then <em>i</em> permutes the
coordinate axes, and thus (using the result of Problem&nbsp;5) acts on them
by some signed permutation matrix&nbsp;<em>A</em>;
with a bit of mathematical induction we then show that <em>i</em>
acts on all of <nobr><strong>R</strong><sup><em>n</em></sup></nobr> by the same&nbsp;<em>A</em>.
<br>
[There are other ways to use the structure of the sets
<nobr><em>T</em>(<em>p</em>,<em>q</em>)</nobr> to reach this conclusion.]



<p>

<img src="redball.gif">
<A HREF="p3.pdf">Problem sets 3 and 4</A>: Metric topology cont'd
<br>
<img src="purpball.gif">
Yes, in problem 10i all vector spaces are over
</nobr><strong>F</strong>=<strong>R</strong> or <strong>C</strong></nobr>.
(We already know from last term it's not true 
over&nbsp;<strong>Q</strong>&hellip;)
<!--
<br>
<img src="purpball.gif">
And yes, in Hint for the 13th (final) problem should say that a
<em>continuous</em> function&nbsp;<em>f</em> is determined by
the preimages of <nobr>[<em>s</em>,1]</nobr> with <em>s</em>
ranging over a dense subset of the unit interval.
NOT!
-->
<br>
<img src="greenball.gif"> Problem 8 is the Lebesgue Covering Lemma
(handout VI already let this cat out of the bag); <em>r</em> is a
&ldquo;Lebesgue number&rdquo; for the open cover.
Problem&nbsp;10(ii) is <nobr>A-5</nobr> on the 1999 Putnam exam
(which specified <nobr><em>n</em> = 1999 &mdash; hah).</nobr>
Problem&nbsp;13 is Urysohn's Lemma.

<P>

<img src="redball.gif">
<A HREF="p5.pdf">Problem set 5</A>: Topology grand finale
<br>
<img src="greenball.gif">
Problem 3 is the proof Rudin gives in Chapter&nbsp;8, pages 184-185.
The proof we outlined last term is Exercises 26 and 27 on pages 202.
<br>
Problem&nbsp;8 is the
<a href="http://en.wikipedia.org/wiki/Arzela-Ascoli_theorem"
target="_blank">Arzel&agrave;-Ascoli theorem</a>.
(It turns out to also be largely given in Rudin Chapter&nbsp;7,
see 7.22 ff.)  The Wikipedia page gives some typical applications
to real and complex analysis.

<P>

<img src="redball.gif">
<A HREF="p6.pdf">Problem set 6</A>: Univariate differential calculus
<br>
<img src="purpball.gif">
In problem 2 the relevant vector spaces are all finite-dimensional,
so all norms are equivalent and differentiability etc. is well&nbsp;defined.
Also, <nobr>&ldquo;<em>f</em>&thinsp; <font size="-1">o</font> <em>g</em>&rdquo;</nobr>
is the function taking <em>x</em> to the composition of the
linear operators
<nobr><em>f</em>&thinsp;(<em>x</em>): <em>V</em> &rarr; <em>W</em></nobr>
and <nobr><em>g</em>(<em>x</em>): <em>U</em> &rarr; <em>V</em></nobr>,
<u>not</u> composition of functions as in the Chain&nbsp;Rule.
<br> <img src="purpball.gif">
Belated <strong>correction</strong> 16.iii.11: in problem 2(ii),
the map
<nobr><em>t</em> &rarr; <em>f</em>&thinsp;(<em>t</em>)<sup>&minus;1</sup></nobr>
is indeed defined and differentiable in some neighborhood
of&nbsp;<em>x</em>, but not necessarily on all of
<nobr>[<em>a</em>, <em>b</em>]</nobr>.
(And its derivative at&nbsp;<em>x</em> is given by the formula
<nobr>
  &minus;<em>f</em>&thinsp;(<em>x</em>)<sup>&minus;1</sup><em>f</em>&thinsp;'(<em>x</em>)&thinsp;<em>f</em>&thinsp;(<em>x</em>)<sup>&minus;1</sup>,
</nobr>
which nicely extends the familiar
<nobr>
  &minus;<em>f</em>&thinsp;(<em>x</em>)<sup>&minus;2</sup><em>f</em>&thinsp;'(<em>x</em>)
</nobr>
for a nonzero scalar-valued function in a way consistent with
the behavior of the derivative and inverse of the matrix transpose.)
<!-- L.Alpoge -->

<P>

<img src="redball.gif">
<A HREF="p7.pdf">Problem set 7</A>: Univariate integral calculus
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 6.iii.11:
yes, this problem set is #7, not #2&hellip;
<br>
<img src="greenball.gif">The nice solution to Problem 6
is to use a Riemann sum associated to a partition by a
geometric series, rather than the usual arithmetic series.

<P>

<img src="redball.gif">
<A HREF="p8.pdf">Problem set 8</A>: Univariate integral calculus, cont'd
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 23.iii.11:
In problem 1, the series for <nobr><em>E</em>(<em>x</em>)</nobr>
starts at <nobr><em>n</em>=0</nobr>, not <nobr><em>n</em>=1</nobr>.
(To be sure if you did the <nobr><em>n</em>=1</nobr> version then
the <nobr><em>n</em>=0</nobr> answer is immediate&hellip;)
<!-- L.Alpoge -->
<br>
<img src="purpball.gif">
Belated <strong>correction</strong> 25.iii.11:
In problem 2, <em>x</em><sub>0</sub> is in
the ground field&nbsp;<em>k</em>,
not the &ldquo;function field&rdquo;&nbsp;<em>K</em>.
<!-- L.Alpoge again -->

<P>

<img src="redball.gif">
<A HREF="p9.pdf">Problem set 9</A>: Multivariate differential calculus
<br>
<img src="purpball.gif">
<strong>Tweaked</strong> 26.iii.11: added parentheses in Problem 7
around the <nobr>&ldquo;<em>n</em>+1&rdquo;</nobr> in
<nobr>&ldquo;(<em>n</em>+1)-dimensional&rdquo;</nobr>
(and yes it's a &ldquo;subspace&rdquo; in the topological sense
but not the vector-space sense&hellip;);
<!-- the latter answering a query from L.Alpoge -->
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 28.iii.11 to fix a typo
(missing &ldquo;a&rdquo; in &ldquo;[Cauchy-Riemann] equations&rdquo;),
to state explicitly in Problem&nbsp;7 that the root
<nobr><em>t</em><sub>0</sub></nobr> is real,
<!-- the latter again answering a query from L.Alpoge -->
and to change the eigenvector of&nbsp;<em>M</em> from
<nobr><em>v</em><sub>0</sub></nobr> to
<nobr><em>v</em>(<em>M</em>)</nobr> in Problem&nbsp;8.
<!-- this correction also from L.Alpoge -->

<P>

<img src="redball.gif">
<A HREF="p10.pdf">Problem set 10</A>: Multivariate integral calculus, etc.
<br>
<font color="aqua">&Delta;</font> [Not to be confused with
<A HREF="p10_af.pdf">this version</A> of PS10; note the due dates.
But if you find a complex number&nbsp;<em>s</em> of real part
&lt;1 such that
<nobr>&sum;<sub><em>n</em>&thinsp;</sub>&mu;(<em>n</em>)&thinsp;<em>n<sup>&minus;s</sup></em></nobr>
converges, please let me know!]
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 7.iv.11 to fix a typo in the generalization
of problem 7: the sum of the
<nobr><em>x<sub>i</sub><sup>s<sub>i</sub></sup></em></nobr>
should be <nobr>&le;1</nobr>, not <nobr>=1</nobr>.
<!-- L.Alpoge -->

<P>

<img src="redball.gif">
<A HREF="p11.pdf">Problem set 11</A>:
Convexity, multivariate change of variables, etc.
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 10.iv.11: the two Rudin problems from p.290
<!-- were already assinged last week :-( so omitted. -->
were already assigned last week :-( so omitted.
<!-- G.Yang; typo correction from "assinged" later noted by L.Alpoge -->
<br>
<img src="purpball.gif">
<strong>Corrected</strong> 14.iv.11: in the last problem, the shell has
<nobr>|<em>x</em>|<nobr> in
<nobr>(<em>r</em><sub>1</sub>, <em>r</em><sub>2</sub>)</nobr>, not
<nobr>(<em>a</em>, <em>b</em>)</nobr>.
<!-- L.Alpoge -->
<br>
<img src="purpball.gif"> and belatedly <strong>Corrected</strong> 25.iv.11:
the numerator on the right-hand side of the displayed equation (in
Problem&nbsp;5) is
<nobr>&Gamma;(&nu;+1)</nobr>, not <nobr>&Gamma;(<em>v</em>+1)</nobr>
[with &ldquo;nu&rdquo;, not &ldquo;vee&rdquo;].
<br>
<img src="purpball.gif">
and very belatedly :-( <strong>Corrected</strong> 7.v.11:
in Problem&nbsp;5 again, it's &nu; that must exceed
<nobr>|&lambda;|</nobr>, not just <nobr>|&nu;|</nobr>.
(If &nu; is sufficiently negative the integral doesn't even converge.)
<!-- L.Alpoge -->
And yes, the hypothesis on &nu; is needlessly strong
(it's a relic of an earlier incorrect formula for the integral)
but if you did it already for <nobr>&nu; &gt; |&lambda;|</nobr>
then you needn't re-do the proof for a wider range of&nbsp;&nu;.
<!-- also L.Alpoge -->
<P>

<img src="redball.gif">
<A HREF="p12.pdf">Problem set 12</A>:
Complex analysis I
<br>
<img src="purpball.gif">
Belatedly <strong>Corrected</strong> 22.iv.11:
meromorphic function<em>s</em>, plural, in Problem&nbsp;7;
and <nobr>|1&minus;<em>x<sub>k</sub></em>|</nobr>,
not <nobr>(1&minus;<em>x<sub>k</sub></em>)</nobr>,
in each factor of the numerator displayed in Problem&nbsp;9

<!-- both L.Alpoge -->

<P>

<img src="orangeball.gif">
<A NAME="current_homework">
<A HREF="p13.pdf">13th and last problem set</A>:
Complex analysis II: residues and contour integration
<br>
<img src="purpball.gif">
Belatedly <strong>Corrected</strong> 29.iv.11, but only
to fix a transparent grammar error in the preamble
<!-- both L.Alpoge -->
